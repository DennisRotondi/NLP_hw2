{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2022 - Homework 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for a fast processing of data and experiments execution for the second homework of the course Natural Language Processing 2022. It has been completely wrote by Dennis Rotondi 1834864 using the methodologies learned throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# imports and deterministic stuff\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\")) #to access hw2 functions\n",
    "sys.path.append(os.path.join(\"../..\")) #to access model folder\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "from utils import read_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "wandb.require(\"service\")\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "_ = pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the bonus exercise and hw1, I want to start by looking at the data I have to better understand how to proceed in the pre-processing operations. I've read that there are problems with some (sentence-ground truth) pairs, since we are not allowed to do any change I'll directly discharge them for the training phase if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_file = \"../../data/EN/train.json\"\n",
    "\n",
    "sentences, labels = read_dataset(en_file)\n",
    "# I'm just playing with the field of a sentence_id to understand our data samples, I've it also in the homework slides\n",
    "# but I want to have a reference here\n",
    "sentence_id = '1996/a/50/18_supp__323:5'\n",
    "print(\"## SENTENCE {} ##\".format(sentence_id))\n",
    "for key in sentences[sentence_id]:\n",
    "    print(key)\n",
    "    print(sentences[sentence_id][key])\n",
    "print(\"## LABEL ##\")\n",
    "for key in labels[sentence_id]:\n",
    "    print(key)\n",
    "    print(labels[sentence_id][key])\n",
    "\n",
    "#let's check and count the different frames and roles\n",
    "verbatlas_frames = Counter()\n",
    "predicate_roles = Counter()\n",
    "\n",
    "for k in labels:\n",
    "    verbatlas_frames.update(labels[k]['predicates'])\n",
    "    for idx in labels[k]['roles']:\n",
    "        predicate_roles.update(labels[k]['roles'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## VF ##\")\n",
    "print(verbatlas_frames)\n",
    "#list of frames in the training dataset\n",
    "l_vf = list(verbatlas_frames.keys())\n",
    "print(l_vf)\n",
    "print(len(l_vf))\n",
    "print(\"## RL ##\")\n",
    "print(predicate_roles)\n",
    "p_r = list(predicate_roles.keys())\n",
    "print(p_r)\n",
    "print(len(p_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are clearly not using all the 466 [verbatlas](https://verbatlas.org/) frames but less than 3/4 of them: 303. Working with fewer clusters surely increases the overall performances because the system can only focus on a subset of them. In the next code cell I want to check if in the dev-set I do not have other frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences, dev_labels = read_dataset(\"../../data/EN/dev.json\")\n",
    "for k in dev_labels:\n",
    "    verbatlas_frames.update(dev_labels[k]['predicates'])\n",
    "    for idx in dev_labels[k]['roles']:\n",
    "        predicate_roles.update(dev_labels[k]['roles'][idx])\n",
    "\n",
    "l_vf_dev = list(verbatlas_frames.keys())\n",
    "print(len(l_vf_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are only 4 more frames in the dev_set wrt the train_set, this information is useful for further consideration when I'll deal with the optional part of this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I'm starting to understand the samples, it's clear that our dataset does not need much pre-processing, since we already have words tokens and associated lemmas for each sentence. Some more useful statistics are on how long are the sentences on average, how many predicates they have and how the distribution of pos-tagging tokens correlate with roles and predicates. I'll rapidly compute them in what follows. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'need_train': True, 'batch_size': 3, 'n_cpu': 8, 'language_model_name': 'bert-base-uncased', 'lr': 0.001, 'wd': 1e-05, 'embedding_dim': 768, 'hidden_dim': 512, 'bidirectional': True, 'num_layers': 5, 'dropout': 0.4, 'trainable_embeddings': True, 'role_classes': 27}\n"
     ]
    }
   ],
   "source": [
    "from datasets_srl import SRL_DataModule\n",
    "from implementation import HParams, SRL_34\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "from utils import read_dataset\n",
    "from mergedeep import merge\n",
    "hparams = asdict(HParams())\n",
    "print(hparams)\n",
    "dev_sentences, dev_labels = read_dataset(\"../../data/EN/dev.json\")\n",
    "train_sentences, train_labels = read_dataset(\"../../data/EN/train.json\")\n",
    "task = \"34\"\n",
    "sentences = merge(train_labels, train_sentences)\n",
    "sentences_test = merge(dev_labels, dev_sentences)\n",
    "data = SRL_DataModule(hparams, task, sentences, sentences_test)\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1996,  2231,  6449,  2008,  5622,  8945,  7507,  2080,  1010,\n",
      "          5622, 11865, 10932,  1010,  9033, 12022,  7507,  2080,  1010,  5003,\n",
      "          1062, 20908,  2078,  1998, 19004,  1062,  4048,  5369,  2022, 15389,\n",
      "          2006,  2382,  2238,  2786,  2044,  2022, 20462,  1997,  4319, 21877,\n",
      "         21814,  1012,   102, 15389,  3102,   102,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2837,  7297, 16755,  2008,  1996,  2110,  2283,  8627,\n",
      "          1996,  2865,  2000,  2622,  1037,  3893,  3746,  1997,  2450,  1998,\n",
      "          1997,  1996,  5020,  3570,  1998,  5368,  1997,  2450,  1998,  2158,\n",
      "          1999,  1996,  2797,  1998,  2270, 10336,  1012,   102,  2622, 19818,\n",
      "           102,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "             0,     0,     0,     0,     0,     0,     0],\n",
      "        [  101,  1996,  2236,  3320,  2203,  5668,  2063,  1996,  7211,  8170,\n",
      "          1998,  4132,  2005,  2895,  1010,  1998,  5323,  1037,  2093,  1011,\n",
      "          7563,  6970,  3995, 23062, 26901,  7337,  1010,  8676,  1997,  1996,\n",
      "          2236,  3320,  1010,  1996,  3171,  1998,  2591,  2473,  1998,  1996,\n",
      "          3222,  2006,  1996,  3570,  1997,  2308,  1010,  2000,  2377,  1996,\n",
      "          3078,  2535,  1999,  3452,  3343,  1011,  2437,  1998,  3582,  1011,\n",
      "          2039,  1010,  1998,  1999, 13530,  1996,  7375,  1998,  8822,  1997,\n",
      "          1996,  4132,  2005,  2895,  1012,   102,  2203,  5668,  2063,  3582,\n",
      "          1035,  2490,  1035, 10460,  1035,  4636,   102]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]), 'id': tensor([ 399, 4188, 4998]), 'word_id': tensor([[31, 31,  1,  2,  3,  4,  5,  5,  5,  6,  7,  8,  8,  9, 10, 11, 11, 11,\n",
      "         12, 13, 14, 14, 14, 15, 16, 17, 17, 17, 18, 19, 20, 21, 22, 23, 24, 25,\n",
      "         26, 27, 28, 29, 29, 30, 31,  0,  1, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "         31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31,\n",
      "         31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31, 31],\n",
      "        [36, 36,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
      "         17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
      "         35, 36,  0,  1, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "         36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36,\n",
      "         36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36, 36],\n",
      "        [69, 69,  1,  2,  3,  3,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14,\n",
      "         15, 16, 17, 18, 18, 18, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,\n",
      "         30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47,\n",
      "         48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65,\n",
      "         66, 67, 68, 69,  0,  0,  0,  1,  1,  1,  1,  1,  1,  1, 69]]), 'labels': tensor([[  26,   26,   26,   26,    3,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [  26,   26,   26,   26,   26,   26,   26,   26,   26,   26,    0,   26,\n",
      "           26,   26,   26,    1,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [  26,   26,    0,   26,   26,   26,    2,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,   26,\n",
      "           26,   26,   26,   26,   26,   26,   26,   26,   26]])}\n",
      "dict_keys(['input_ids', 'token_type_ids', 'attention_mask', 'id', 'word_id', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "data.setup()\n",
    "batch = iter(data.train_dataloader()).next()\n",
    "print(batch)\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = SRL_34(hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"word_id\"].dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 69, 27])\n",
      "torch.Size([3, 69])\n"
     ]
    }
   ],
   "source": [
    "print(model(batch).shape)\n",
    "print(batch[\"labels\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dennis/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "ename": "MisconfigurationException",
     "evalue": "GPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMisconfigurationException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[1;32m/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb Cell 19'\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=8'>9</a>\u001b[0m early_stop_callback \u001b[39m=\u001b[39m EarlyStopping(monitor\u001b[39m=\u001b[39mmetric_to_monitor, min_delta\u001b[39m=\u001b[39m\u001b[39m0.00\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m15\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, mode\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=9'>10</a>\u001b[0m checkpoint_callback \u001b[39m=\u001b[39m ModelCheckpoint(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=10'>11</a>\u001b[0m                         save_top_k\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=11'>12</a>\u001b[0m                         monitor \u001b[39m=\u001b[39m metric_to_monitor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=15'>16</a>\u001b[0m                         verbose \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=16'>17</a>\u001b[0m                     )\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=17'>18</a>\u001b[0m trainer \u001b[39m=\u001b[39m pl\u001b[39m.\u001b[39;49mTrainer(logger \u001b[39m=\u001b[39;49m wandb_logger,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=18'>19</a>\u001b[0m                     max_epochs \u001b[39m=\u001b[39;49m epochs, \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=19'>20</a>\u001b[0m                     gpus\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=20'>21</a>\u001b[0m                     callbacks\u001b[39m=\u001b[39;49m[early_stop_callback, checkpoint_callback])    \n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=21'>22</a>\u001b[0m \u001b[39m# Start the training\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/dennis/Desktop/nlp2022-hw2/hw2/stud/nlp_hw2.ipynb#ch0000032?line=22'>23</a>\u001b[0m trainer\u001b[39m.\u001b[39mfit(model,data)\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/utilities/argparse.py:339\u001b[0m, in \u001b[0;36m_defaults_from_env_vars.<locals>.insert_env_defaults\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    336\u001b[0m kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\u001b[39mlist\u001b[39m(env_variables\u001b[39m.\u001b[39mitems()) \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(kwargs\u001b[39m.\u001b[39mitems()))\n\u001b[1;32m    338\u001b[0m \u001b[39m# all args were already moved to kwargs\u001b[39;00m\n\u001b[0;32m--> 339\u001b[0m \u001b[39mreturn\u001b[39;00m fn(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/trainer.py:483\u001b[0m, in \u001b[0;36mTrainer.__init__\u001b[0;34m(self, logger, checkpoint_callback, enable_checkpointing, callbacks, default_root_dir, gradient_clip_val, gradient_clip_algorithm, process_position, num_nodes, num_processes, devices, gpus, auto_select_gpus, tpu_cores, ipus, log_gpu_memory, progress_bar_refresh_rate, enable_progress_bar, overfit_batches, track_grad_norm, check_val_every_n_epoch, fast_dev_run, accumulate_grad_batches, max_epochs, min_epochs, max_steps, min_steps, max_time, limit_train_batches, limit_val_batches, limit_test_batches, limit_predict_batches, val_check_interval, flush_logs_every_n_steps, log_every_n_steps, accelerator, strategy, sync_batchnorm, precision, enable_model_summary, weights_summary, weights_save_path, num_sanity_val_steps, resume_from_checkpoint, profiler, benchmark, deterministic, reload_dataloaders_every_n_epochs, auto_lr_find, replace_sampler_ddp, detect_anomaly, auto_scale_batch_size, prepare_data_per_node, plugins, amp_backend, amp_level, move_metrics_to_cpu, multiple_trainloader_mode, stochastic_weight_avg, terminate_on_nan)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39m# init connectors\u001b[39;00m\n\u001b[1;32m    481\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_connector \u001b[39m=\u001b[39m DataConnector(\u001b[39mself\u001b[39m, multiple_trainloader_mode)\n\u001b[0;32m--> 483\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_connector \u001b[39m=\u001b[39m AcceleratorConnector(\n\u001b[1;32m    484\u001b[0m     num_processes\u001b[39m=\u001b[39;49mnum_processes,\n\u001b[1;32m    485\u001b[0m     devices\u001b[39m=\u001b[39;49mdevices,\n\u001b[1;32m    486\u001b[0m     tpu_cores\u001b[39m=\u001b[39;49mtpu_cores,\n\u001b[1;32m    487\u001b[0m     ipus\u001b[39m=\u001b[39;49mipus,\n\u001b[1;32m    488\u001b[0m     accelerator\u001b[39m=\u001b[39;49maccelerator,\n\u001b[1;32m    489\u001b[0m     strategy\u001b[39m=\u001b[39;49mstrategy,\n\u001b[1;32m    490\u001b[0m     gpus\u001b[39m=\u001b[39;49mgpus,\n\u001b[1;32m    491\u001b[0m     num_nodes\u001b[39m=\u001b[39;49mnum_nodes,\n\u001b[1;32m    492\u001b[0m     sync_batchnorm\u001b[39m=\u001b[39;49msync_batchnorm,\n\u001b[1;32m    493\u001b[0m     benchmark\u001b[39m=\u001b[39;49mbenchmark,\n\u001b[1;32m    494\u001b[0m     replace_sampler_ddp\u001b[39m=\u001b[39;49mreplace_sampler_ddp,\n\u001b[1;32m    495\u001b[0m     deterministic\u001b[39m=\u001b[39;49mdeterministic,\n\u001b[1;32m    496\u001b[0m     auto_select_gpus\u001b[39m=\u001b[39;49mauto_select_gpus,\n\u001b[1;32m    497\u001b[0m     precision\u001b[39m=\u001b[39;49mprecision,\n\u001b[1;32m    498\u001b[0m     amp_type\u001b[39m=\u001b[39;49mamp_backend,\n\u001b[1;32m    499\u001b[0m     amp_level\u001b[39m=\u001b[39;49mamp_level,\n\u001b[1;32m    500\u001b[0m     plugins\u001b[39m=\u001b[39;49mplugins,\n\u001b[1;32m    501\u001b[0m )\n\u001b[1;32m    502\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_logger_connector \u001b[39m=\u001b[39m LoggerConnector(\u001b[39mself\u001b[39m, log_gpu_memory)\n\u001b[1;32m    503\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_callback_connector \u001b[39m=\u001b[39m CallbackConnector(\u001b[39mself\u001b[39m)\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:196\u001b[0m, in \u001b[0;36mAcceleratorConnector.__init__\u001b[0;34m(self, devices, num_nodes, accelerator, strategy, plugins, precision, amp_type, amp_level, sync_batchnorm, benchmark, replace_sampler_ddp, deterministic, auto_select_gpus, num_processes, tpu_cores, ipus, gpus)\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mauto\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    195\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_flag \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_accelerator()\n\u001b[0;32m--> 196\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_parallel_devices_and_init_accelerator()\n\u001b[1;32m    198\u001b[0m \u001b[39m# 3. Instantiate ClusterEnvironment\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster_environment: ClusterEnvironment \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_choose_and_init_cluster_environment()\n",
      "File \u001b[0;32m~/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/trainer/connectors/accelerator_connector.py:514\u001b[0m, in \u001b[0;36mAcceleratorConnector._set_parallel_devices_and_init_accelerator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39mis_available():\n\u001b[1;32m    511\u001b[0m     available_accelerator \u001b[39m=\u001b[39m [\n\u001b[1;32m    512\u001b[0m         acc_str \u001b[39mfor\u001b[39;00m acc_str \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_accelerator_types \u001b[39mif\u001b[39;00m AcceleratorRegistry\u001b[39m.\u001b[39mget(acc_str)\u001b[39m.\u001b[39mis_available()\n\u001b[1;32m    513\u001b[0m     ]\n\u001b[0;32m--> 514\u001b[0m     \u001b[39mraise\u001b[39;00m MisconfigurationException(\n\u001b[1;32m    515\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__qualname__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m can not run on your system\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    516\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m since the accelerator is not available. The following accelerator(s)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    517\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is available and can be passed into `accelerator` argument of\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    518\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m `Trainer`: \u001b[39m\u001b[39m{\u001b[39;00mavailable_accelerator\u001b[39m}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    519\u001b[0m     )\n\u001b[1;32m    521\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_devices_flag_if_auto_passed()\n\u001b[1;32m    523\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_devices_flag \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gpus\n",
      "\u001b[0;31mMisconfigurationException\u001b[0m: GPUAccelerator can not run on your system since the accelerator is not available. The following accelerator(s) is available and can be passed into `accelerator` argument of `Trainer`: ['cpu']."
     ]
    }
   ],
   "source": [
    "# Define the logger\n",
    "# https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "wandb_logger = WandbLogger(project=\"VAE WM\", log_model=True)\n",
    "wandb_logger.experiment.watch(model, log='all', log_freq=1000)\n",
    "# Define the trainer\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "metric_to_monitor = 'avg_val_loss_vae'#\"loss\"\n",
    "early_stop_callback = EarlyStopping(monitor=metric_to_monitor, min_delta=0.00, patience=15, verbose=True, mode=\"min\")\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "                        save_top_k=1,\n",
    "                        monitor = metric_to_monitor,\n",
    "                        mode = \"min\",\n",
    "                        dirpath = \"../../model\",\n",
    "                        filename =\"vae-{epoch:02d}-{avg_val_loss_vae:.4f}\",\n",
    "                        verbose = True\n",
    "                    )\n",
    "trainer = pl.Trainer(logger = wandb_logger,\n",
    "                    max_epochs = epochs, \n",
    "                    gpus=1,\n",
    "                    callbacks=[early_stop_callback, checkpoint_callback])    \n",
    "# Start the training\n",
    "trainer.fit(model,data)\n",
    "# # Log the trained model\n",
    "# trainer.save_checkpoint(\"../../model\")\n",
    "wandb.save(\"../../wandb_logs\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOREMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "sequence2 = \"we ciao\"\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# # print(tokens)\n",
    "\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# # print(ids)\n",
    "\n",
    "tokenized_inputs = tokenizer([[sequence, \"simple\"],[sequence2, \"ciao\"]],padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens\n",
    "\n",
    "print(tokenized_inputs)\n",
    "print(tokenized_inputs['input_ids'].shape)\n",
    "print(tokenized_inputs.word_ids(0))\n",
    "print(tokenized_inputs.word_ids(1))\n",
    "# NOTE: in the dataset use those word ids to average and simply filter for example... MAY NOT WORK:...\n",
    "sequence3 = [\"we\", \"ciao\"]\n",
    "print(\"second - use this one!!\")\n",
    "tokenized_inputs2 = tokenizer.batch_encode_plus([([sequence.split(), [\"simple\"]]),(sequence3,[\"ciao\"])], add_special_tokens=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens\n",
    "print(tokenized_inputs2)\n",
    "print(tokenized_inputs2['input_ids'].shape)\n",
    "print(tokenized_inputs2.word_ids(0))\n",
    "a, b, c = tokenizer.batch_encode_plus([([sequence.split(), [\"simple\"]]),(sequence3,[\"ciao\"])], add_special_tokens=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens \n",
    "print(\"aaa\")\n",
    "print(b)\n",
    "print(\"aaa\")\n",
    "# print(tokenized_inputs2.word_ids(1))\n",
    "# sequence_a = \"This is a short sequence.\"\n",
    "# sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "# print(\"test\")\n",
    "# padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
    "# print(padded_sequences)\n",
    "\n",
    "transformers_outputs = auto_model(**tokenized_inputs)#['input_ids']\n",
    "# print(transformers_outputs)\n",
    "transformers_outputs_sum = torch.stack(transformers_outputs.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "print(transformers_outputs_sum.shape)\n",
    "# I should remove 2 sep and 1 cls, 1 additional token -> final size 7\n",
    "\n",
    "\n",
    "\n",
    "# filter_toke = tokenized_inputs['input_ids'][:, 1:-3, ...]\n",
    "# print(filter_toke.shape)\n",
    "# labels  = tokenized_inputs.word_ids()[1:-3]\n",
    "# samp_size = filter_toke.shape[1]\n",
    "# M = torch.zeros(max(labels)+1, samp_size)\n",
    "# M[labels, torch.arange(samp_size)] = 1\n",
    "# print(M)\n",
    "# M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "# print(M)\n",
    "# torch.mm(M, filter_toke[0]).shape\n",
    "\n",
    "# i want to have an ID to unde\n",
    "# item[\"role_id\"] = (item[\"role_labels\"] == self.labels_to_id[\"_\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.Tensor([[\n",
    "                     [0.1, 0.1],    #-> group / class 1\n",
    "                     [0.2, 0.2],    #-> group / class 2\n",
    "                     [0.4, 0.4],    #-> group / class 2\n",
    "                     [0.0, 0.0]     #-> group / class 0\n",
    "              ],\n",
    "              [\n",
    "                     [0.1, 0.1],    #-> group / class 1\n",
    "                     [0.2, 0.2],    #-> group / class 1\n",
    "                     [0.0, 0.0],    #-> group / class 0\n",
    "                     [12.0, 12.0]   #-> group / class 0\n",
    "              ]])\n",
    "\n",
    "from transformers_embedder.embedder import TransformersEmbedder\n",
    "labels = torch.LongTensor([[1, 2, 2, 2],[1,2,0,2]])\n",
    "\n",
    "\n",
    "print(TransformersEmbedder.merge_scatter(samples, labels))\n",
    "print(TransformersEmbedder.merge_scatter(samples, labels).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from Riccardo Orlando transformer embedding https://github.com/Riccorl/transformers-embedder\n",
    "# it is needed to average the wordpieces after the tokenization to have more reliable embeddig. This is \n",
    "# useful because for OOV words (or other languages) we can capture more informations than simply using\n",
    "# the first token. \n",
    "def merge_scatter(embeddings: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Minimal version of ``scatter_mean``, from `pytorch_scatter\n",
    "    <https://github.com/rusty1s/pytorch_scatter/>`_\n",
    "    library, that is compatible for ONNX but works only for our case.\n",
    "    It is used to compute word level embeddings from the transformer output.\n",
    "    Args:\n",
    "        embeddings (:obj:`torch.Tensor`):\n",
    "            The embeddings tensor.\n",
    "        indices (:obj:`torch.Tensor`):\n",
    "            The sub-word indices.\n",
    "    Returns:\n",
    "        :obj:`torch.Tensor`\n",
    "    \"\"\"\n",
    "\n",
    "    def broadcast(src: torch.Tensor, other: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Broadcast ``src`` to match the shape of ``other``.\n",
    "        Args:\n",
    "            src (:obj:`torch.Tensor`):\n",
    "                The tensor to broadcast.\n",
    "            other (:obj:`torch.Tensor`):\n",
    "                The tensor to match the shape of.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: The broadcasted tensor.\n",
    "        \"\"\"\n",
    "        for _ in range(src.dim(), other.dim()):\n",
    "            src = src.unsqueeze(-1)\n",
    "        src = src.expand_as(other)\n",
    "        return src\n",
    "\n",
    "    def scatter_sum(src: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sums the elements in ``src`` that have the same indices as in ``index``.\n",
    "        Args:\n",
    "            src (:obj:`torch.Tensor`):\n",
    "                The tensor to sum.\n",
    "            index (:obj:`torch.Tensor`):\n",
    "                The indices to sum.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: The summed tensor.\n",
    "        \"\"\"\n",
    "        index = broadcast(index, src)\n",
    "        size = list(src.size())\n",
    "        size[1] = index.max() + 1\n",
    "        print(size)\n",
    "        print(src.dtype)\n",
    "        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n",
    "        return out.scatter_add_(1, index, src)\n",
    "\n",
    "    # replace padding indices with the maximum value inside the batch\n",
    "    indices[indices == -1] = torch.max(indices)\n",
    "    merged = scatter_sum(embeddings, indices)\n",
    "    ones = torch.ones(\n",
    "        indices.size(), dtype=embeddings.dtype, device=embeddings.device\n",
    "    )\n",
    "    count = scatter_sum(ones, indices)\n",
    "    count.clamp_(1)\n",
    "    count = broadcast(count, merged)\n",
    "    merged.true_divide_(count)\n",
    "    return merged[:,:-1,:] #added by me to remove a batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,None],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers-embedder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eed754111ede77ce2654f5ed00c707307144a8607362a4447b4b2089c46effd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
