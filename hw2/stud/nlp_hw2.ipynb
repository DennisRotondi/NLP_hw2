{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2022 - Homework 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for a fast processing of data and experiments execution for the second homework of the course Natural Language Processing 2022. It has been completely wrote by Dennis Rotondi 1834864 using the methodologies learned throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and deterministic stuff\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\")) #to access hw2 functions\n",
    "sys.path.append(os.path.join(\"../..\")) #to access model folder\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = './nlp_hw2.ipynb' # to avoid a wandb warning\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\" # to avoid deadlock at traning time for the tokenizer\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import read_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "_ = pl.seed_everything(0)\n",
    "\n",
    "# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "# these commands allow to update the .py codes imported instead of re-importing everything every time.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the bonus exercise and hw1, I want to start by looking at the data I have to better understand how to proceed in the pre-processing operations. I've read that there are problems with some (sentence-ground truth) pairs, since we are not allowed to do any change I'll directly discharge them for the training phase if needed. I'll do my analysis mostly for the english dataset since it is mandatory and larger \n",
    "\n",
    "(it's possible to reproduce the experiments in different languages just changing the language parameter that follow from \"EN\" to {\"ES\", \"FR\"})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"EN\" # \"ES\" or \"FR\" if you want\n",
    "data_file = f\"../../data/{language}/train.json\"\n",
    "\n",
    "sentences, labels = read_dataset(data_file)\n",
    "print(f\"Number of training sentences ({language}): \"+ str(len(sentences.keys())))\n",
    "# I'm just playing with the field of a sentence_id to understand our data samples.\n",
    "sentence_id = list(sentences.keys())[0]\n",
    "print(\"## SENTENCE {} ##\".format(sentence_id))\n",
    "for key in sentences[sentence_id]:\n",
    "    print(key)\n",
    "    print(sentences[sentence_id][key])\n",
    "print(\"## LABEL ##\")\n",
    "for key in labels[sentence_id]:\n",
    "    print(key)\n",
    "    print(labels[sentence_id][key])\n",
    "\n",
    "# let's check and count the different frames and roles\n",
    "verbatlas_frames = Counter()\n",
    "predicate_roles = Counter()\n",
    "pos_tags = Counter()\n",
    "\n",
    "for k in labels:\n",
    "    verbatlas_frames.update(labels[k]['predicates'])\n",
    "    pos_tags.update(sentences[k]['pos_tags'])\n",
    "    for idx in labels[k]['roles']:\n",
    "        predicate_roles.update(labels[k]['roles'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## VF ##\")\n",
    "print(verbatlas_frames)\n",
    "# list of frames in the training dataset\n",
    "l_vf = list(verbatlas_frames.keys())\n",
    "print(l_vf)\n",
    "print(len(l_vf))\n",
    "print(\"## RL ##\")\n",
    "print(predicate_roles)\n",
    "p_r = list(predicate_roles.keys())\n",
    "print(p_r)\n",
    "print(len(p_r))\n",
    "print(\"## PT ##\")\n",
    "ppl = list(pos_tags.keys())\n",
    "print(ppl)\n",
    "print(len(ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(predicate_roles[\"_\"])\n",
    "# to place it in a different position and not have overlapping labels\n",
    "predicate_roles[\"experiencer\"] = predicate_roles.pop(\"experiencer\")\n",
    "plt.figure(figsize=(26,10))\n",
    "_ = plt.bar(predicate_roles.keys(), predicate_roles.values()) \n",
    "plt.title(\"Bar Plot of role frequency (without '_' label, 95% of them)\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are clearly not using all the 466 [verbatlas](https://verbatlas.org/) frames but less than 3/4 of them: 303. Working with fewer clusters surely increases the overall performances because the system can only focus on a subset of them. In the next code cell I want to check if in the dev-set I do not have other frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences, dev_labels = read_dataset(f\"../../data/{language}/dev.json\")\n",
    "print(f\"Number of training sentences ({language}): \"+ str(len(dev_sentences.keys())))\n",
    "for k in dev_labels:\n",
    "    verbatlas_frames.update(dev_labels[k]['predicates'])\n",
    "    for idx in dev_labels[k]['roles']:\n",
    "        predicate_roles.update(dev_labels[k]['roles'][idx])\n",
    "\n",
    "l_vf_dev = list(verbatlas_frames.keys())\n",
    "print(len(l_vf_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are only 4 more frames in the dev_set wrt the train_set, this information is useful for further consideration when I'll deal with the optional part of this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I'm starting to understand the samples, it's clear that our dataset does not need much pre-processing, since we already have words tokens and associated lemmas for each sentence. Some more useful statistics are on how long are the sentences on average, how many predicates they have and how the distribution of pos-tagging tokens correlate with roles and predicates. I'll rapidly compute them in what follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mergedeep import merge\n",
    "sentences_length = list()\n",
    "predicates_counter = list()\n",
    "sentences = merge(sentences, labels)\n",
    "pos_pre_corr = Counter()\n",
    "pos_role_corr = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tags = list(pos_tags.keys())\n",
    "for pt in pos_tags:\n",
    "    pos_pre_corr.update({pt:0})\n",
    "    pos_role_corr.update({pt:0})\n",
    "\n",
    "for s in sentences:\n",
    "    s_l = len(sentences[s][\"lemmas\"])\n",
    "    roles = sentences[s][\"roles\"].keys()\n",
    "    p_c = len(roles)\n",
    "    sentences_length.append(s_l)\n",
    "    predicates_counter.append(p_c)\n",
    "    for pos, predicate in zip(sentences[s][\"pos_tags\"], sentences[s][\"predicates\"]):\n",
    "        if predicate != \"_\":\n",
    "            pos_pre_corr.update({pos:1})\n",
    "    for r in roles:\n",
    "        for pos, role in zip(sentences[s][\"pos_tags\"],sentences[s][\"roles\"][r]):\n",
    "            if role != \"_\":\n",
    "                pos_role_corr.update({pos:1})\n",
    "    \n",
    "sl_np=np.asarray(sentences_length)\n",
    "pc_np=np.asarray(predicates_counter)\n",
    "\n",
    "print(\"Sentences Length\")\n",
    "print(\"mean\", sl_np.mean())\n",
    "print(\"std\", sl_np.std())\n",
    "print(\"min\", sl_np.min())\n",
    "print(\"max\", sl_np.max())\n",
    "\n",
    "print(\"Predicates Counter\")\n",
    "print(\"mean\", pc_np.mean())\n",
    "print(\"std\", pc_np.std())\n",
    "print(\"min\", pc_np.min())\n",
    "print(\"max\", pc_np.max())\n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "_ = plt.hist(sl_np, bins = 'auto') \n",
    "plt.title(\"Histogram of sentences length available\") \n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "_ = plt.hist(pc_np, bins = 5) \n",
    "plt.title(\"Histogram of predicate counts for each sentence\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to notice that (for our EN dataset) there are some sentences with 0 and some with 10 predicates, even if the average is slightly more than 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26,10))\n",
    "_ = plt.bar(pos_pre_corr.keys(), pos_pre_corr.values()) \n",
    "plt.title(\"Number of predicates for each pos_token\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30\n",
    "\n",
    "print(pos_pre_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A photo is worth a thousand words, if we are able to identify the pos_tag it's also very easy to understand we have a verb or not. (holy grail for task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26,10))\n",
    "_ = plt.bar(pos_role_corr.keys(), pos_role_corr.values()) \n",
    "plt.title(\"Number of predicates for each pos_token\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30\n",
    "\n",
    "print(pos_role_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case it's clear that there are pos tags that gives more information about which are the arguments, so I'll introduce them in my model if time will allow it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood the importance of having a postag information, taking into account that the input sentence on which my work will be evaluated does not have pos-tag information, I have to retrieve them with an external library. It is important to understand how good is this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from seqeval.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "taggers = {\"EN\":\"en_core_web_sm\", \"ES\":\"es_core_news_sm\", \"FR\":\"fr_core_news_sm\"}\n",
    "nlp = spacy.load(taggers[language])\n",
    "\n",
    "def compute_metrics_postag(field: str):\n",
    "    p_labels = list()\n",
    "    predictions = list()\n",
    "    ppl = list()\n",
    "    pv = list()\n",
    "    for s in tqdm(sentences):\n",
    "        fr = ' '.join(sentences[s][field])\n",
    "        doc = nlp(fr)\n",
    "        for token, pos in zip(doc,sentences[s][\"pos_tags\"]):\n",
    "            predictions.append(token.pos_)\n",
    "            p_labels.append(pos)\n",
    "            if pos ==  \"VERB\":\n",
    "                pv.append(token.pos_)\n",
    "                ppl.append(pos)\n",
    "    acc = accuracy_score([p_labels], [predictions])\n",
    "    f = f1_score([p_labels], [predictions])\n",
    "    print(\"Accuracy, f1 on all the tokens\")\n",
    "    print(acc, f)\n",
    "    # but as seen to solve task 1 we are more interested in identify verb tokens!\n",
    "    accv = accuracy_score([ppl], [pv])\n",
    "    fv = f1_score([ppl], [pv])\n",
    "    print(\"Accuracy on VERB tokens\")\n",
    "    print(accv)\n",
    "\n",
    "compute_metrics_postag(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we repeat the same experiment computing the pos_tag from the lemmas and NOT the words\n",
    "compute_metrics_postag(\"lemmas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that since acc for verbs with words is much greater than the one with lemmas is better to embed using words. To be precise we are not using the most accurate spacy model for pos-tagging, still this is the most efficient (30mb instead of 400mb) and for this homework I'm not aiming to \"top score\" but to complete different pipelines in a reasonable time due to the fact that working with transformers requires a lot of resources in term of memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model. Pytorch-lightning allows that in such a way that it's easy to modularize everything and train with few lines of code all the different models. Moreover using wandb as logger I auto-plot the training evolution in high quality plots and it's also possible to save the training history of the different trials. This will be very useful for comparing the experiments in the report.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_srl import SRL_DataModule\n",
    "from implementation import HParams, SRL_34, SRL_234, SRL_1234\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "from utils import read_dataset, evaluate_argument_classification, evaluate_argument_identification\n",
    "from mergedeep import merge\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# these are some parameters that allow as I said to modularize the training. We need to store the hypermarameters of the model (lr, wd, ...), the language\n",
    "# and the task on which we want to perform the training.\n",
    "hparams = HParams()\n",
    "languages = [\"EN\", \"ES\", \"FR\"]\n",
    "tasks = [\"34\", \"234\", \"1234\"]\n",
    "models = {\"34\": SRL_34, \"234\": SRL_234, \"1234\": SRL_1234}\n",
    "\n",
    "language = languages[0]\n",
    "task = tasks[2]\n",
    "epochs = 100\n",
    "SRL_Model = models[task]\n",
    "hparams.language = language\n",
    "hparams.task = task\n",
    "hparams = asdict(hparams)\n",
    "pprint(hparams)\n",
    "# after reading the dataset I merge the two dicts (sentences and labels) since there is a field in common (predicate)\n",
    "# and it's only a waste of space keeping it in memory 2 copies of it.\n",
    "sentences = merge(*read_dataset(\"../../data/\"+language+\"/train.json\"))\n",
    "sentences_test = merge(*read_dataset(\"../../data/\"+language+\"/dev.json\"))\n",
    "\n",
    "data = SRL_DataModule(hparams, task, language, sentences, sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SRL_Model(hparams=hparams, sentences_for_evaluation=sentences_test)\n",
    "# Define the logger\n",
    "# https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "# NOTE: to use wandb properly you need to login in wandb (need an account) \n",
    "# or use a different logger eg. TensorBoard, I'm used to this one so I'll go for it.\n",
    "# to login: https://docs.wandb.ai/ref/cli/wandb-login\n",
    "wandb.require(\"service\")\n",
    "wandb_logger = WandbLogger(project=\"SRL_\"+task, log_model = True) # note not language in the project name, so we can compare different languages\n",
    "wandb_logger.experiment.watch(model, log = 'False', log_freq = 100000)\n",
    "# Define the trainer\n",
    "metric_to_monitor =  \"f1\" # f1 of argument classification, also possible eg 'avg_val_loss'\n",
    "mode = \"max\" #you want to maximixe or minimize the metric?\n",
    "# we employ the early stopping technique to avoid hours of usuless training, pl gives it for free\n",
    "early_stop_callback = EarlyStopping(monitor = metric_to_monitor, min_delta = 0.00, patience = 5, verbose = True, mode = mode)\n",
    "# it is also useful to keep track of the best model during the epochs (if you remember I did all this manually last hw)or use a different logger,\n",
    "# we have a callback even for this.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "                        save_top_k = 1,\n",
    "                        monitor = metric_to_monitor,\n",
    "                        mode = mode,\n",
    "                        dirpath = \"../../model\",\n",
    "                        filename = \"SRL_\"+task+\"_\"+language+\"-{epoch:02d}-{f1:.4f}\",\n",
    "                        verbose = True\n",
    "                    )\n",
    "# the trainer collect all the useful informations so far for the training \n",
    "trainer = pl.Trainer(logger = wandb_logger,\n",
    "                    max_epochs = epochs, \n",
    "                    gpus = 1,\n",
    "                    callbacks = [early_stop_callback, checkpoint_callback])    \n",
    "\n",
    "save_ckpt_file = \"../../model/SRL_{}_{}_last.ckpt\".format(task, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training without initialized weights, if you want to inizialize them skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data)\n",
    "trainer.save_checkpoint(save_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue the training it's possible to just increase the number of epochs and create a new trainer, also possible to fine tune the model for another language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_ckpt = save_ckpt_file # or use another language / pre-trained model.\n",
    "epochs += 10 # increase the maximum number of epochs\n",
    "trainer = pl.Trainer(logger = wandb_logger,\n",
    "                    max_epochs = epochs, \n",
    "                    gpus = 1,\n",
    "                    callbacks = [early_stop_callback, checkpoint_callback],\n",
    "                    resume_from_checkpoint = resume_ckpt)    \n",
    "trainer.fit(model, data)\n",
    "trainer.save_checkpoint(save_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have trained for enough epochs you can now finish logging with wandb to have your plot.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload online the run after finishing (you will have a string like the one below, just execute in on terminal or here)\n",
    "!wandb sync /home/dennis/Desktop/nlp2022-hw2/hw2/stud/wandb/offline-run-20220712_105118-2uum41pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis for argument classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trained model it's interesting to understand where it performs better and where not. Recalling that from the english dataset analysis these were the numbers of different roles:\n",
    "\n",
    "{'_': 437392, 'agent': 7581, 'theme': 6593, 'patient': 2907, 'goal': 1463, 'topic': 1403, 'recipient': 837, 'beneficiary': 590, 'result': 577, 'stimulus': 367, 'experiencer': 319, 'attribute': 294, 'destination': 276, 'co-theme': 253, 'source': 229, 'location': 198, 'co-agent': 145, 'product': 95, 'instrument': 70, 'co-patient': 60, 'extent': 54, 'cause': 51, 'value': 45, 'time': 35, 'asset': 28, 'purpose': 25, 'material': 11}\n",
    "\n",
    "by inspection could be possible to estimate a correlation between the number of samples and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and utils for this evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from typing import Dict, List, Any\n",
    "from utils import evaluate_argument_classification, evaluate_argument_identification\n",
    "\n",
    "def flat_dict_roles(sentences: Dict[str, Dict[str, List[str]]]) -> List[Any]:\n",
    "    list_tokens = list()\n",
    "    for s in sentences:\n",
    "        for l in sentences[s][\"roles\"]:\n",
    "            list_tokens+=[token for token in sentences[s][\"roles\"][l]]\n",
    "    return list_tokens\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt = \"../../model/SRL_34_EN-epoch=55-f1=0.8581.ckpt\"\n",
    "# we load weights in a not strict fashion since I made some adjustment to the models during the way \n",
    "model = SRL_34.load_from_checkpoint(best_ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.to(device).predict(sentences_test, require_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI\")\n",
    "print(evaluate_argument_identification(sentences_test, predict))\n",
    "print(\"AC\")\n",
    "print(evaluate_argument_classification(sentences_test, predict))\n",
    "\n",
    "flat_labels_s = flat_dict_roles(sentences_test)\n",
    "flat_predictions_s = flat_dict_roles(predict)\n",
    "\n",
    "all_labels = ['_', 'agent', 'theme', 'beneficiary', 'patient', 'topic', 'goal', 'recipient', 'co-theme', 'result', \\\n",
    "    'stimulus', 'experiencer', 'destination', 'value', 'attribute', 'location', 'source', 'cause', 'co-agent', \\\n",
    "    'time', 'co-patient', 'product', 'purpose', 'instrument', 'extent', 'asset', 'material']\n",
    "\n",
    "cm = confusion_matrix(flat_labels_s, flat_predictions_s, labels=all_labels, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=all_labels)\n",
    "fig, ax = plt.subplots(figsize=(36,36))\n",
    "disp.plot(ax=ax) #7162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the fewer the samples the worst the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 234 - AMuSE-WSD extra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since for task 1(234) and task 34 I've basically implemented a very robust architecture that for some time had the SOTA results, it's clear that repeating something similar for 234 will lead to best results. Instead of doing a boring almost copy-paste of the other models where this time we could replace the post-tag with the pre-computed WSD predicts or simply the predicate identification, I've decided to first try a different approach using the suggested architecture AMuSE-WSD that currently has SOTA for WSD in multiple languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What I decided to do is indeed a fun but sub-optimal procedure: I start by predicting the meaning with AMuSE-WSD that uses more frames that the ones we have in the english dataset (the one I'll use as reference), so at this point I fine tune the predictions, also to remove the OOV prediction, to the most common real values for the training dataset, in a certain sense we are fine-tuning a model by applying a \"filter layer\" to increase the predictions! \n",
    "\n",
    " For doing so I'll use all the different dataset informations since this model is intended to be used on different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from amuse import AMuSE_WSD_online\n",
    "from utils import evaluate_predicate_disambiguation\n",
    "\n",
    "# extracted above from english dataset analysis\n",
    "frames_in_dataset = ['_', 'ASK_REQUEST', 'BENEFIT_EXPLOIT', 'PLAN_SCHEDULE', 'CARRY-OUT-ACTION', 'ESTABLISH', 'SIMPLIFY', 'PROPOSE', 'TAKE-INTO-ACCOUNT_CONSIDER', 'BEGIN', 'CIRCULATE_SPREAD_DISTRIBUTE', 'REFER', 'SHOW', 'PRECLUDE_FORBID_EXPEL', 'VIOLATE', 'VERIFY', 'CAUSE-SMT', 'ABSTAIN_AVOID_REFRAIN', 'TRANSMIT', 'SEE', 'SUMMON', 'GUARANTEE_ENSURE_PROMISE', 'RECEIVE', 'INCREASE_ENLARGE_MULTIPLY', 'DECREE_DECLARE', 'PAY', 'CAUSE-MENTAL-STATE', 'CAGE_IMPRISON', 'HURT_HARM_ACHE', 'MOVE-BACK', 'EXIST_LIVE', 'CALCULATE_ESTIMATE', 'ATTRACT_SUCK', 'EXIST-WITH-FEATURE', 'INFORM', 'EXPLAIN', 'SPEAK', 'SEEM', 'MISS_OMIT_LACK', 'DECIDE_DETERMINE', 'ASSIGN-SMT-TO-SMN', 'FOLLOW_SUPPORT_SPONSOR_FUND', 'MOVE-ONESELF', 'WORSEN', 'AMELIORATE', 'AGREE_ACCEPT', 'MOVE-SOMETHING', 'PUT_APPLY_PLACE_PAVE', 'ADJUST_CORRECT', 'INCLUDE-AS', 'CONTINUE', 'SPEED-UP', 'LOAD_PROVIDE_CHARGE_FURNISH', 'REMEMBER', 'FINISH_CONCLUDE_END', 'REPEAT', 'HELP_HEAL_CARE_CURE', 'IMPLY', 'OPPOSE_REBEL_DISSENT', 'STRENGTHEN_MAKE-RESISTANT', 'AROUSE_WAKE_ENLIVEN', 'RECORD', 'INCITE_INDUCE', 'GIVE_GIFT', 'DESTROY', 'REQUIRE_NEED_WANT_HOPE', 'ANALYZE', 'COME-AFTER_FOLLOW-IN-TIME', 'BELIEVE', 'GO-FORWARD', 'CANCEL_ELIMINATE', 'RECOGNIZE_ADMIT_IDENTIFY', 'CHOOSE', 'REPRESENT', 'TREAT', 'OBLIGE_FORCE', 'STOP', 'REACT', 'HAPPEN_OCCUR', 'OVERCOME_SURPASS', 'AFFECT', 'CREATE_MATERIALIZE', 'ALLY_ASSOCIATE_MARRY', 'MANAGE', 'OPEN', 'ORIENT', 'ANSWER', 'INFLUENCE', 'COMBINE_MIX_UNITE', 'LEAD_GOVERN', 'STAY_DWELL', 'WELCOME', 'AMASS', 'PREPARE', 'ORGANIZE', 'HAVE-A-FUNCTION_SERVE', 'GIVE-UP_ABOLISH_ABANDON', 'SORT_CLASSIFY_ARRANGE', 'GIVE-BIRTH', 'PUBLISH', 'USE', 'POSSESS', 'BEHAVE', 'WORK', 'SUBJECTIVE-JUDGING', 'APPROVE_PRAISE', 'ATTEND', 'LEAVE_DEPART_RUN-AWAY', 'CATCH', 'OBEY', 'SATISFY_FULFILL', 'UNDERSTAND', 'ACHIEVE', 'TRY', 'ATTACH', 'INTERPRET', 'DELAY', 'REDUCE_DIMINISH', 'UNDERGO-EXPERIENCE', 'RETAIN_KEEP_SAVE-MONEY', 'ARRIVE', 'REFUSE', 'IMAGINE', 'HARMONIZE', 'PARTICIPATE', 'HIRE', 'RESULT_CONSEQUENCE', 'FOCUS', 'CONTAIN', 'MOUNT_ASSEMBLE_PRODUCE', 'PROVE', 'WRITE', 'RESTRAIN', 'TOLERATE', 'ACCOMPANY', 'DISCUSS', 'RESTORE-TO-PREVIOUS/INITIAL-STATE_UNDO_UNWIND', 'TEACH', 'CHANGE-APPEARANCE/STATE', 'INVERT_REVERSE', 'RELY', 'SIGNAL_INDICATE', 'LEARN', 'ACCUSE', 'PERFORM', 'AFFIRM', 'REMOVE_TAKE-AWAY_KIDNAP', 'WATCH_LOOK-OUT', 'GROUND_BASE_FOUND', 'LEAVE-BEHIND', 'FACE_CHALLENGE', 'CHANGE_SWITCH', 'SHARE', 'APPLY', 'ARGUE-IN-DEFENSE', 'DIRECT_AIM_MANEUVER', 'WAIT', 'HEAR_LISTEN', 'CONSIDER', 'LIKE', 'FIGHT', 'PROTECT', 'AUTHORIZE_ADMIT', 'DIVERSIFY', 'PRESERVE', 'LOCATE-IN-TIME_DATE', 'SEND', 'ORDER', 'SEARCH', 'REGRET_SORRY', 'EMPHASIZE', 'CELEBRATE_PARTY', 'TAKE-SHELTER', 'HOST_MEAL_INVITE', 'REPLACE', 'THINK', 'MEET', 'PERCEIVE', 'BREAK_DETERIORATE', 'JOIN_CONNECT', 'BORDER', 'FIND', 'KNOW', 'KILL', 'CHARGE', 'FAIL_LOSE', 'CRITICIZE', 'CITE', 'HIT', 'LIBERATE_ALLOW_AFFORD', 'BRING', 'DERIVE', 'JUSTIFY_EXCUSE', 'PERSUADE', 'REVEAL', 'DRIVE-BACK', 'TAKE', 'OBTAIN', 'LOSE', 'ADD', 'MATCH', 'CONSUME_SPEND', 'COMPARE', 'BEFRIEND', 'NAME', 'BE-LOCATED_BASE', 'OFFER', 'OVERLAP', 'CARRY_TRANSPORT', 'REACH', 'FILL', 'ENCLOSE_WRAP', 'DISBAND_BREAK-UP', 'COUNT', 'DEFEAT', 'CO-OPT', 'ENDANGER', 'PUNISH', 'TRANSLATE', 'SECURE_FASTEN_TIE', 'INSERT', 'REMAIN', 'BUY', 'STEAL_DEPRIVE', 'SETTLE_CONCILIATE', 'EXTEND', 'SUMMARIZE', 'PUBLICIZE', 'CORRELATE', 'SEPARATE_FILTER_DETACH', 'GROUP', 'COST', 'ATTACK_BOMB', 'WARN', 'NEGOTIATE', 'ENTER', 'LIE', 'SPEND-TIME_PASS-TIME', 'EMPTY_UNLOAD', 'INVERT_REVERSE-', 'EMIT', 'TURN_CHANGE-DIRECTION', 'SELL', 'GUESS', 'DISCARD', 'CONTRACT-AN-ILLNESS_INFECT', 'WASH_CLEAN', 'DROP', 'OPERATE', 'SHARPEN', 'REFLECT', 'COMPENSATE', 'ASCRIBE', 'LOWER', 'COPY', 'DEBASE_ADULTERATE', 'DISMISS_FIRE-SMN', 'COVER_SPREAD_SURMOUNT', 'MEASURE_EVALUATE', 'RESIGN_RETIRE', 'READ', 'DISTINGUISH_DIFFER', 'TRAVEL', 'RESIST', 'SHOOT_LAUNCH_PROPEL', 'BURDEN_BEAR', 'SOLVE', 'WIN', 'APPEAR', 'FOLLOW-IN-SPACE', 'PULL', 'PAINT', 'COME-FROM', 'VISIT', 'COOL', 'DOWNPLAY_HUMILIATE', 'CHASE', 'EMBELLISH', 'EARN', 'RAISE', 'PROMOTE', 'MEAN', 'EXHAUST', 'ABSORB', 'PRESS_PUSH_FOLD', 'LEND', 'SHAPE', 'PRINT', 'REPAIR_REMEDY', 'GROW_PLOW', 'QUARREL_POLEMICIZE', 'TAKE-A-SERVICE_RENT', 'COMPETE', 'DIVIDE', 'COMMUNICATE_CONTACT', 'FIT', 'EXEMPT', 'SLOW-DOWN', 'FLOW', 'RISK', 'METEOROLOGICAL', 'NOURISH_FEED', 'STABILIZE_SUPPORT-PHYSICALLY']\n",
    "mod = AMuSE_WSD_online(language, filter_layer=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filter = dict()\n",
    "# uncomment if you have slow internet like me\n",
    "# it is the result of mod.predict(sentences, require_ids=True) using words\n",
    "# the online pipeline is better working with words than lemmas because it is tought to be a full end-to-end state-of-the-art \n",
    "# multilingual pretrained model.\n",
    "# pred = torch.load(\"../../model/amuse/predictions_words\") \n",
    "\n",
    "# comment if you have slow internet\n",
    "pred = mod.predict(sentences, require_ids=True)\n",
    "\n",
    "print(evaluate_predicate_disambiguation(pred, sentences))\n",
    "# oov_labels = 0\n",
    "possibilities = dict()\n",
    "for p in pred:\n",
    "    for predict, real in zip(pred[p][\"predicates\"], sentences[p][\"predicates\"]):\n",
    "        possibilities[predict] = possibilities.get(predict,{})\n",
    "        possibilities[predict][real] = possibilities[predict].get(real,0) + 1\n",
    "# if oov_labels == 0:\n",
    "#     # we end the training, we have all inside our labels_vocabulary\n",
    "#     break\n",
    "for k in possibilities:\n",
    "    max_count = 0\n",
    "    max_label = \"\"\n",
    "    for j in possibilities[k]:\n",
    "        if possibilities[k][j] > max_count:\n",
    "            max_count = possibilities[k][j]\n",
    "            max_label = j\n",
    "    filter[k] = max_label\n",
    "# code to avoid spending time in recumputing everything with mod.predict\n",
    "new_pred = dict()\n",
    "for k in pred:\n",
    "    new_pred[k] = {\"predicates\" : [filter[s] for s in pred[k][\"predicates\"]]}\n",
    "print(evaluate_predicate_disambiguation(new_pred, sentences))\n",
    "\"\"\"\n",
    "RESULTS:\n",
    "{'true_positives': 8838, 'false_positives': 3724, 'false_negatives': 2878, \n",
    "'precision': 0.7035503900652762, 'recall': 0.7543530215090475, 'f1': 0.7280665623197956}\n",
    "{'true_positives': 9229, 'false_positives': 3333, 'false_negatives': 2487, \n",
    "'precision': 0.7346760070052539, 'recall': 0.7877261864117446, 'f1': 0.7602767938050911}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we conclude with this section repeating the experiment with and without the filter for the dev_set, then we save all these predictions and filter to work with a \"fine tuned\" amuse-wsd. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = mod.predict(sentences_test, require_ids=True)\n",
    "print(evaluate_predicate_disambiguation(pred_test, sentences_test))\n",
    "mod.filter = filter\n",
    "new_pred_test = mod.predict(sentences_test, require_ids=True)\n",
    "print(evaluate_predicate_disambiguation(new_pred_test, sentences_test))\n",
    "\"\"\"\n",
    "RESULTS:\n",
    "{'true_positives': 1782, 'false_positives': 771, 'false_negatives': 546, \n",
    "'precision': 0.6980023501762632, 'recall': 0.7654639175257731, 'f1': 0.7301782421634909}\n",
    "{'true_positives': 1841, 'false_positives': 712, 'false_negatives': 487, \n",
    "'precision': 0.7211124167645907, 'recall': 0.790807560137457, 'f1': 0.7543536160622824}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice index 2 wrt the one I upload... I was scared to override them\n",
    "torch.save(pred, \"../../model/amuse/prediction_words2\")\n",
    "torch.save(pred_test, \"../../model/amuse/prediction_words_dev2\")\n",
    "torch.save(new_pred, \"../../model/amuse/prediction_words_new2\")\n",
    "torch.save(new_pred_test, \"../../model/amuse/prediction_words_dev_new2\")\n",
    "torch.save(filter, \"../../model/amuse/filter_layer2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOREMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = torch.load(\"../../model/SRL_234_layer.ckpt\")\n",
    "print(len(frames_in_dataset))\n",
    "print(filt)\n",
    "print(len(filt.keys()))\n",
    "mod.filter = filt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\"words\": sentences[\"1996/a/50/18_supp__323:5\"][\"words\"], \"lemmas\": sentences[\"1996/a/50/18_supp__323:5\"][\"lemmas\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_srl import SRL_DataModule\n",
    "from implementation import HParams, SRL_1234, SRL_34\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "from utils import read_dataset, evaluate_argument_classification, evaluate_argument_identification\n",
    "from mergedeep import merge\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "while True:\n",
    "    # these are some parameters that allow as I said to modularize the training. We need to store the hypermarameters of the model (lr, wd, ...), the language\n",
    "    # and the task on which we want to perform the training.\n",
    "    hparams = HParams()\n",
    "    languages = [\"EN\", \"ES\", \"FR\"]\n",
    "    tasks = [\"34\", \"234\", \"1234\"]\n",
    "    models = {\"34\": SRL_34, \"1234\": SRL_1234}\n",
    "\n",
    "    language = languages[0]\n",
    "    task = tasks[2]\n",
    "    epochs = 100\n",
    "    SRL_Model = models[task]\n",
    "    hparams.language = language\n",
    "    hparams.task = task\n",
    "    hparams = asdict(hparams)\n",
    "    pprint(hparams)\n",
    "    # after reading the dataset I merge the two dicts (sentences and labels) since there is a field in common (predicate)\n",
    "    # and it's only a waste of space keeping it in memory 2 copies of it.\n",
    "    sentences = merge(*read_dataset(\"../../data/\"+language+\"/train.json\"))\n",
    "    sentences_test = merge(*read_dataset(\"../../data/\"+language+\"/dev.json\"))\n",
    "\n",
    "    data = SRL_DataModule(hparams, task, language, sentences, sentences_test)\n",
    "    model = SRL_Model(hparams=hparams, sentences_for_evaluation=sentences_test)\n",
    "    # Define the logger\n",
    "    # https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "    # NOTE: to use wandb properly you need to login in wandb (need an account) \n",
    "    # or use a different logger eg. TensorBoard, I'm used to this one so I'll go for it.\n",
    "    # to login: https://docs.wandb.ai/ref/cli/wandb-login\n",
    "    wandb.require(\"service\")\n",
    "    wandb_logger = WandbLogger(project=\"SRL_\"+task, log_model = True) # note not language in the project name, so we can compare different languages\n",
    "    wandb_logger.experiment.watch(model, log = 'False', log_freq = 100000)\n",
    "    # Define the trainer\n",
    "    metric_to_monitor =  \"f1\" # f1 of argument classification, also possible eg 'avg_val_loss'\n",
    "    mode = \"max\" #you want to maximixe or minimize the metric?\n",
    "    # we employ the early stopping technique to avoid hours of usuless training, pl gives it for free\n",
    "    early_stop_callback = EarlyStopping(monitor = metric_to_monitor, min_delta = 0.00, patience = 15, verbose = True, mode = mode)\n",
    "    # it is also useful to keep track of the best model during the epochs (if you remember I did all this manually last hw)or use a different logger,\n",
    "    # we have a callback even for this.\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            save_top_k = 1,\n",
    "                            monitor = metric_to_monitor,\n",
    "                            mode = mode,\n",
    "                            dirpath = \"../../model\",\n",
    "                            filename = \"SRL_\"+task+\"_\"+language+\"-{epoch:02d}-{f1:.4f}\",\n",
    "                            verbose = True\n",
    "                        )\n",
    "    # the trainer collect all the useful informations so far for the training \n",
    "    trainer = pl.Trainer(logger = wandb_logger,\n",
    "                        max_epochs = epochs, \n",
    "                        gpus = 1,\n",
    "                        callbacks = [early_stop_callback, checkpoint_callback])    \n",
    "\n",
    "    save_ckpt_file = \"../../model/SRL_{}_{}_last.ckpt\".format(task, language)\n",
    "    trainer.fit(model, data)\n",
    "    wandb.finish()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"../../model/srl_34_EN.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(flat_labels_s, flat_predictions_s, labels=all_labels, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=all_labels)\n",
    "fig, ax = plt.subplots(figsize=(400,400))\n",
    "disp.plot(ax=ax) #7162"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eed754111ede77ce2654f5ed00c707307144a8607362a4447b4b2089c46effd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
