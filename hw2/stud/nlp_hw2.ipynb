{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2022 - Homework 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for a fast processing of data and experiments execution for the second homework of the course Natural Language Processing 2022. It has been completely wrote by Dennis Rotondi 1834864 using the methodologies learned throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "# imports and deterministic stuff\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\")) #to access hw2 functions\n",
    "sys.path.append(os.path.join(\"../..\")) #to access model folder\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = './nlp_hw2.ipynb' # to avoid a wandb warning\n",
    "# os.environ['TOKENIZERS_PARALLELISM'] = \"false\" # to avoid deadlock at traning time\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from collections import OrderedDict, Counter\n",
    "\n",
    "from utils import read_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "_ = pl.seed_everything(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the bonus exercise and hw1, I want to start by looking at the data I have to better understand how to proceed in the pre-processing operations. I've read that there are problems with some (sentence-ground truth) pairs, since we are not allowed to do any change I'll directly discharge them for the training phase if needed. I'll do my analysis mostly for the english dataset since it is mandatory and larger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_file = \"../../data/EN/train.json\"\n",
    "\n",
    "sentences, labels = read_dataset(en_file)\n",
    "print(\"Number of training sentences (EN): \"+ str(len(sentences.keys())))\n",
    "# I'm just playing with the field of a sentence_id to understand our data samples.\n",
    "sentence_id = '1996/a/50/18_supp__323:5'\n",
    "print(\"## SENTENCE {} ##\".format(sentence_id))\n",
    "for key in sentences[sentence_id]:\n",
    "    print(key)\n",
    "    print(sentences[sentence_id][key])\n",
    "print(\"## LABEL ##\")\n",
    "for key in labels[sentence_id]:\n",
    "    print(key)\n",
    "    print(labels[sentence_id][key])\n",
    "\n",
    "# let's check and count the different frames and roles\n",
    "verbatlas_frames = Counter()\n",
    "predicate_roles = Counter()\n",
    "\n",
    "for k in labels:\n",
    "    verbatlas_frames.update(labels[k]['predicates'])\n",
    "    for idx in labels[k]['roles']:\n",
    "        predicate_roles.update(labels[k]['roles'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## VF ##\")\n",
    "print(verbatlas_frames)\n",
    "# list of frames in the training dataset\n",
    "l_vf = list(verbatlas_frames.keys())\n",
    "print(l_vf)\n",
    "print(len(l_vf))\n",
    "print(\"## RL ##\")\n",
    "print(predicate_roles)\n",
    "p_r = list(predicate_roles.keys())\n",
    "print(p_r)\n",
    "print(len(p_r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are clearly not using all the 466 [verbatlas](https://verbatlas.org/) frames but less than 3/4 of them: 303. Working with fewer clusters surely increases the overall performances because the system can only focus on a subset of them. In the next code cell I want to check if in the dev-set I do not have other frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences, dev_labels = read_dataset(\"../../data/EN/dev.json\")\n",
    "print(\"Number of training sentences (EN): \"+ str(len(dev_sentences.keys())))\n",
    "for k in dev_labels:\n",
    "    verbatlas_frames.update(dev_labels[k]['predicates'])\n",
    "    for idx in dev_labels[k]['roles']:\n",
    "        predicate_roles.update(dev_labels[k]['roles'][idx])\n",
    "\n",
    "l_vf_dev = list(verbatlas_frames.keys())\n",
    "print(len(l_vf_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are only 4 more frames in the dev_set wrt the train_set, this information is useful for further consideration when I'll deal with the optional part of this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I'm starting to understand the samples, it's clear that our dataset does not need much pre-processing, since we already have words tokens and associated lemmas for each sentence. Some more useful statistics are on how long are the sentences on average, how many predicates they have and how the distribution of pos-tagging tokens correlate with roles and predicates. I'll rapidly compute them in what follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_size=len(tokens_s)\n",
    "sentences_size=list()\n",
    "k=0\n",
    "for s,l in zip(tokens_s,labels_s):\n",
    "    sentences_size.append(len(s))\n",
    "    for w,lab in zip(s,l):\n",
    "        if not w in vocab and lab!=\"O\":\n",
    "            k+=1\n",
    "print(\"important words lost\",k)\n",
    "k=0\n",
    "for s,l in zip(tokens_s,labels_s):\n",
    "    for w,lab in zip(s,l):\n",
    "        if not w in vocab and lab!=\"O\":\n",
    "            k+=1\n",
    "            break\n",
    "print(\"percentage of dirty sentences\",k/token_size) #sentences that contains an OOV word but with a significant label !=0\n",
    "\n",
    "sent_np=np.asarray(sentences_size)\n",
    "print(\"mean\", sent_np.mean())\n",
    "print(\"std\", sent_np.std())\n",
    "print(\"min\", sent_np.min())\n",
    "print(\"max\", sent_np.max())\n",
    "\n",
    "plt.figure(figsize=(8,8)) #to increase the plot size\n",
    "_ = plt.hist(sent_np, bins = 'auto') \n",
    "plt.title(\"Histogram of sentences size available\") \n",
    "plt.show()\n",
    "\n",
    "flat_labels = sum(labels_s,[]) #to flat the list\n",
    "count = Counter(flat_labels)\n",
    "plt.figure(figsize=(10,10))\n",
    "_ = plt.bar(count.keys(),count.values()) \n",
    "plt.title(\"Bar Plot of labels frequency\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model. Pytorch-lightning allow that in such a way that it's easy to modularize everything and train with few lines of code all the different models. Moreover using wandb as logger I auto-plot the training evolution in high quality plots and it's also possible to save the training history of the different trials. This will be very useful for comparing the experiments in the report.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with notebook need an 'absolute' import\n",
      "{'need_train': True, 'batch_size': 256, 'n_cpu': 8, 'language_model_name': 'bert-base-uncased', 'lr': 0.001, 'wd': 0, 'embedding_dim': 768, 'hidden_dim': 512, 'bidirectional': True, 'num_layers': 1, 'dropout': 0.3, 'trainable_embeddings': False, 'role_classes': 27, 'srl_34_ckpt': 'model/srl_34_EN.ckpt'}\n"
     ]
    }
   ],
   "source": [
    "from datasets_srl import SRL_DataModule\n",
    "from implementation import HParams, SRL_34\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "from utils import read_dataset, evaluate_argument_classification, evaluate_argument_identification\n",
    "from mergedeep import merge\n",
    "\n",
    "# these are some parameters that allow as I said to modularize the training. We need to store the hypermarameters of the model (lr, wd, ...), the language\n",
    "# and the task on which we want to perform the training.\n",
    "hparams = asdict(HParams())\n",
    "print(hparams)\n",
    "languages = [\"EN\", \"ES\", \"FR\"]\n",
    "tasks = [\"34\", \"234\", \"1234\"]\n",
    "models = {\"34\": SRL_34}\n",
    "\n",
    "\n",
    "language = languages[0]\n",
    "task = tasks[0]\n",
    "epochs = 30\n",
    "SRL_Model = models[task]\n",
    "# after reading the dataset I merge the two dicts (sentences and labels) since there is a field in common (predicate)\n",
    "# and it's only a waste of space keeping it in memory 2 copies of it.\n",
    "sentences = merge(*read_dataset(\"../../data/\"+language+\"/train.json\"))\n",
    "sentences_test = merge(*read_dataset(\"../../data/\"+language+\"/dev.json\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "data = SRL_DataModule(hparams, task, sentences, sentences_test)\n",
    "model = SRL_Model(hparams=hparams, sentences_for_evaluation=sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2h52vs2e.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/dennis/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:611: UserWarning: Checkpoint directory /home/dennis/Desktop/nlp2022-hw2/model exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name              | Type      | Params\n",
      "------------------------------------------------\n",
      "0 | transformer_model | BertModel | 109 M \n",
      "1 | lstm              | LSTM      | 5.3 M \n",
      "2 | dropout           | Dropout   | 0     \n",
      "3 | classifier        | Linear    | 27.7 K\n",
      "------------------------------------------------\n",
      "5.3 M     Trainable params\n",
      "109 M     Non-trainable params\n",
      "114 M     Total params\n",
      "459.044   Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.226, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved. New best score: 0.192\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.226, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'avg_val_loss' reached 0.19167 (best 0.19167), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=00-avg_val_loss=0.1917.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60/60 [00:50<00:00,  1.18it/s, loss=0.175, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.042 >= min_delta = 0.0. New best score: 0.150\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 60/60 [00:50<00:00,  1.18it/s, loss=0.175, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1, global step 100: 'avg_val_loss' reached 0.14996 (best 0.14996), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=01-avg_val_loss=0.1500.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 60/60 [00:49<00:00,  1.22it/s, loss=0.141, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.033 >= min_delta = 0.0. New best score: 0.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 60/60 [00:49<00:00,  1.22it/s, loss=0.141, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 150: 'avg_val_loss' reached 0.11714 (best 0.11714), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=02-avg_val_loss=0.1171.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 60/60 [00:47<00:00,  1.27it/s, loss=0.112, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.020 >= min_delta = 0.0. New best score: 0.097\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 60/60 [00:47<00:00,  1.27it/s, loss=0.112, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3, global step 200: 'avg_val_loss' reached 0.09709 (best 0.09709), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=03-avg_val_loss=0.0971.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 60/60 [00:49<00:00,  1.20it/s, loss=0.0952, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.011 >= min_delta = 0.0. New best score: 0.086\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 60/60 [00:49<00:00,  1.20it/s, loss=0.0952, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 250: 'avg_val_loss' reached 0.08634 (best 0.08634), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=04-avg_val_loss=0.0863.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 60/60 [00:51<00:00,  1.17it/s, loss=0.087, v_num=vs2e] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.010 >= min_delta = 0.0. New best score: 0.077\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 60/60 [00:51<00:00,  1.17it/s, loss=0.087, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5, global step 300: 'avg_val_loss' reached 0.07676 (best 0.07676), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=05-avg_val_loss=0.0768.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 60/60 [00:51<00:00,  1.15it/s, loss=0.0747, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.005 >= min_delta = 0.0. New best score: 0.072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 60/60 [00:51<00:00,  1.15it/s, loss=0.0747, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6, global step 350: 'avg_val_loss' reached 0.07187 (best 0.07187), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=06-avg_val_loss=0.0719.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.0703, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.006 >= min_delta = 0.0. New best score: 0.066\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.0703, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 400: 'avg_val_loss' reached 0.06575 (best 0.06575), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=07-avg_val_loss=0.0657.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.0655, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.003 >= min_delta = 0.0. New best score: 0.063\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.0655, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8, global step 450: 'avg_val_loss' reached 0.06272 (best 0.06272), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=08-avg_val_loss=0.0627.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.0583, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.061\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 60/60 [00:51<00:00,  1.16it/s, loss=0.0583, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9, global step 500: 'avg_val_loss' reached 0.06105 (best 0.06105), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=09-avg_val_loss=0.0611.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 60/60 [00:51<00:00,  1.17it/s, loss=0.0551, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.060\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 60/60 [00:51<00:00,  1.17it/s, loss=0.0551, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 550: 'avg_val_loss' reached 0.05974 (best 0.05974), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=10-avg_val_loss=0.0597.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 60/60 [00:52<00:00,  1.15it/s, loss=0.0518, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.002 >= min_delta = 0.0. New best score: 0.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11: 100%|██████████| 60/60 [00:52<00:00,  1.15it/s, loss=0.0518, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 600: 'avg_val_loss' reached 0.05815 (best 0.05815), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=11-avg_val_loss=0.0581.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 60/60 [00:52<00:00,  1.15it/s, loss=0.0472, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.058\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12: 100%|██████████| 60/60 [00:52<00:00,  1.15it/s, loss=0.0472, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 650: 'avg_val_loss' reached 0.05755 (best 0.05755), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=12-avg_val_loss=0.0576.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13: 100%|██████████| 60/60 [00:48<00:00,  1.23it/s, loss=0.0429, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 700: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 60/60 [00:44<00:00,  1.35it/s, loss=0.0408, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.000 >= min_delta = 0.0. New best score: 0.057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14: 100%|██████████| 60/60 [00:44<00:00,  1.35it/s, loss=0.0408, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 750: 'avg_val_loss' reached 0.05725 (best 0.05725), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=14-avg_val_loss=0.0572.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15: 100%|██████████| 60/60 [00:49<00:00,  1.20it/s, loss=0.0389, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 800: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 60/60 [00:45<00:00,  1.31it/s, loss=0.0357, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric avg_val_loss improved by 0.001 >= min_delta = 0.0. New best score: 0.056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16: 100%|██████████| 60/60 [00:45<00:00,  1.31it/s, loss=0.0357, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 850: 'avg_val_loss' reached 0.05621 (best 0.05621), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=16-avg_val_loss=0.0562.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17: 100%|██████████| 60/60 [00:44<00:00,  1.34it/s, loss=0.0359, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 900: 'avg_val_loss' reached 0.05643 (best 0.05621), saving model to '/home/dennis/Desktop/nlp2022-hw2/model/SRL_34-epoch=17-avg_val_loss=0.0564.ckpt' as top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18: 100%|██████████| 60/60 [00:45<00:00,  1.30it/s, loss=0.0344, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 950: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 60/60 [00:54<00:00,  1.09it/s, loss=0.0311, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 1000: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20: 100%|██████████| 60/60 [00:47<00:00,  1.26it/s, loss=0.0296, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 1050: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21: 100%|██████████| 60/60 [00:44<00:00,  1.36it/s, loss=0.029, v_num=vs2e] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21, global step 1100: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22: 100%|██████████| 60/60 [00:44<00:00,  1.36it/s, loss=0.0274, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22, global step 1150: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23: 100%|██████████| 60/60 [00:54<00:00,  1.10it/s, loss=0.0257, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23, global step 1200: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24: 100%|██████████| 60/60 [00:49<00:00,  1.21it/s, loss=0.024, v_num=vs2e] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24, global step 1250: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25: 100%|██████████| 60/60 [00:57<00:00,  1.05it/s, loss=0.023, v_num=vs2e] "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25, global step 1300: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 60/60 [00:48<00:00,  1.24it/s, loss=0.0228, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric avg_val_loss did not improve in the last 10 records. Best score: 0.056. Signaling Trainer to stop.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 60/60 [00:48<00:00,  1.24it/s, loss=0.0228, v_num=vs2e]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26, global step 1350: 'avg_val_loss' was not in top 2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26: 100%|██████████| 60/60 [00:48<00:00,  1.24it/s, loss=0.0228, v_num=vs2e]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>avg_val_loss</td><td>█▆▄▃▃▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▆▃▃▃▂▃▄▃▃▂▂▁▂▂▂▂▂▁▂▁▂▁▁▁▁</td></tr><tr><td>trainer/global_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>avg_val_loss</td><td>0.05962</td></tr><tr><td>epoch</td><td>26</td></tr><tr><td>loss</td><td>0.02714</td></tr><tr><td>trainer/global_step</td><td>1349</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "You can sync this run to the cloud by running:<br/><code>wandb sync /home/dennis/Desktop/nlp2022-hw2/hw2/stud/wandb/offline-run-20220712_024732-2h52vs2e<code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/offline-run-20220712_024732-2h52vs2e/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# Define the logger\n",
    "# https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "# NOTE: to use wandb properly you need to login in wandb (need an account) \n",
    "# or use a different logger eg. TensorBoard, I'm used to this one so I'll go for it.\n",
    "# to login: https://docs.wandb.ai/ref/cli/wandb-login\n",
    "wandb.require(\"service\")\n",
    "wandb_logger = WandbLogger(project=\"SRL_\"+task, log_model = True)\n",
    "wandb_logger.experiment.watch(model, log = 'False', log_freq = 100000)\n",
    "# Define the trainer\n",
    "metric_to_monitor = 'avg_val_loss' # f1 of argument classification \n",
    "# we employ the early stopping technique to avoid hours of usuless training, pl gives it for free\n",
    "early_stop_callback = EarlyStopping(monitor = metric_to_monitor, min_delta = 0.00, patience = 10, verbose = True, mode = \"min\")\n",
    "# it is also useful to keep track of the best model during the epochs (if you remember I did all this manually last hw)or use a different logger,\n",
    "# we have a callback even for this.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "                        save_top_k = 2,\n",
    "                        monitor = metric_to_monitor,\n",
    "                        mode = \"min\",\n",
    "                        dirpath = \"../../model\",\n",
    "                        filename = \"SRL_\"+task+\"-{epoch:02d}-{avg_val_loss:.4f}\",\n",
    "                        verbose = True\n",
    "                    )\n",
    "# the trainer collect all the useful informations so far for the training \n",
    "trainer = pl.Trainer(logger = wandb_logger,\n",
    "                    max_epochs = epochs, \n",
    "                    gpus = 1,\n",
    "                    callbacks = [early_stop_callback, checkpoint_callback])    \n",
    "# Start the training\n",
    "trainer.fit(model, data)\n",
    "# Log the trained model\n",
    "trainer.save_checkpoint(\"../../model/SRL_\"+task+\"_last.ckpt\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload online the run \n",
    "!wandb sync /home/dennis/Desktop/nlp2022-hw2/hw2/stud/wandb/offline-run-20220712_024732-2h52vs2e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "model = SRL_34.load_from_checkpoint(\"../../model/SRL_3402.ckpt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1026/1026 [00:23<00:00, 44.51it/s]\n"
     ]
    }
   ],
   "source": [
    "predict = model.predict(sentences_test, require_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Argument Identification\n",
      "{'true_positives': 4089, 'false_positives': 504, 'false_negatives': 924, 'precision': 0.8902677988242979, 'recall': 0.8156792339916218, 'f1': 0.8513429106808245}\n",
      "Argument Classification\n",
      "{'true_positives': 3862, 'false_positives': 731, 'false_negatives': 1151, 'precision': 0.8408447637709558, 'recall': 0.7703969678835029, 'f1': 0.8040807828440557}\n"
     ]
    }
   ],
   "source": [
    "print(\"Argument Identification\")\n",
    "print(evaluate_argument_identification(sentences_test, predict))\n",
    "print(\"Argument Classification\")\n",
    "print(evaluate_argument_classification(sentences_test, predict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"../../model/srl_34_EN.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: NOW the confusion matrix "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOREMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "sequence2 = \"we ciao\"\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# # print(tokens)\n",
    "\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# # print(ids)\n",
    "\n",
    "tokenized_inputs = tokenizer([[sequence, \"simple\"],[sequence2, \"ciao\"]],padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens\n",
    "\n",
    "print(tokenized_inputs)\n",
    "print(tokenized_inputs['input_ids'].shape)\n",
    "print(tokenized_inputs.word_ids(0))\n",
    "print(tokenized_inputs.word_ids(1))\n",
    "# NOTE: in the dataset use those word ids to average and simply filter for example... MAY NOT WORK:...\n",
    "sequence3 = [\"we\", \"ciao\"]\n",
    "print(\"second - use this one!!\")\n",
    "tokenized_inputs2 = tokenizer.batch_encode_plus([([sequence.split(), [\"simple\"]]),(sequence3,[\"ciao\"])], add_special_tokens=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens\n",
    "print(tokenized_inputs2)\n",
    "print(tokenized_inputs2['input_ids'].shape)\n",
    "print(tokenized_inputs2.word_ids(0))\n",
    "a, b, c = tokenizer.batch_encode_plus([([sequence.split(), [\"simple\"]]),(sequence3,[\"ciao\"])], add_special_tokens=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens \n",
    "print(\"aaa\")\n",
    "print(b)\n",
    "print(\"aaa\")\n",
    "# print(tokenized_inputs2.word_ids(1))\n",
    "# sequence_a = \"This is a short sequence.\"\n",
    "# sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "# print(\"test\")\n",
    "# padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
    "# print(padded_sequences)\n",
    "\n",
    "transformers_outputs = auto_model(**tokenized_inputs)#['input_ids']\n",
    "# print(transformers_outputs)\n",
    "transformers_outputs_sum = torch.stack(transformers_outputs.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "print(transformers_outputs_sum.shape)\n",
    "# I should remove 2 sep and 1 cls, 1 additional token -> final size 7\n",
    "\n",
    "\n",
    "\n",
    "# filter_toke = tokenized_inputs['input_ids'][:, 1:-3, ...]\n",
    "# print(filter_toke.shape)\n",
    "# labels  = tokenized_inputs.word_ids()[1:-3]\n",
    "# samp_size = filter_toke.shape[1]\n",
    "# M = torch.zeros(max(labels)+1, samp_size)\n",
    "# M[labels, torch.arange(samp_size)] = 1\n",
    "# print(M)\n",
    "# M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "# print(M)\n",
    "# torch.mm(M, filter_toke[0]).shape\n",
    "\n",
    "# i want to have an ID to unde\n",
    "# item[\"role_id\"] = (item[\"role_labels\"] == self.labels_to_id[\"_\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.Tensor([[\n",
    "                     [0.1, 0.1],    #-> group / class 1\n",
    "                     [0.2, 0.2],    #-> group / class 2\n",
    "                     [0.4, 0.4],    #-> group / class 2\n",
    "                     [0.0, 0.0]     #-> group / class 0\n",
    "              ],\n",
    "              [\n",
    "                     [0.1, 0.1],    #-> group / class 1\n",
    "                     [0.2, 0.2],    #-> group / class 1\n",
    "                     [0.0, 0.0],    #-> group / class 0\n",
    "                     [12.0, 12.0]   #-> group / class 0\n",
    "              ]])\n",
    "\n",
    "from transformers_embedder.embedder import TransformersEmbedder\n",
    "labels = torch.LongTensor([[1, 2, 2, 2],[1,2,0,2]])\n",
    "\n",
    "\n",
    "print(TransformersEmbedder.merge_scatter(samples, labels))\n",
    "print(TransformersEmbedder.merge_scatter(samples, labels).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code taken from Riccardo Orlando transformer embedding https://github.com/Riccorl/transformers-embedder\n",
    "# it is needed to average the wordpieces after the tokenization to have more reliable embeddig. This is \n",
    "# useful because for OOV words (or other languages) we can capture more informations than simply using\n",
    "# the first token. \n",
    "def merge_scatter(embeddings: torch.Tensor, indices: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Minimal version of ``scatter_mean``, from `pytorch_scatter\n",
    "    <https://github.com/rusty1s/pytorch_scatter/>`_\n",
    "    library, that is compatible for ONNX but works only for our case.\n",
    "    It is used to compute word level embeddings from the transformer output.\n",
    "    Args:\n",
    "        embeddings (:obj:`torch.Tensor`):\n",
    "            The embeddings tensor.\n",
    "        indices (:obj:`torch.Tensor`):\n",
    "            The sub-word indices.\n",
    "    Returns:\n",
    "        :obj:`torch.Tensor`\n",
    "    \"\"\"\n",
    "\n",
    "    def broadcast(src: torch.Tensor, other: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Broadcast ``src`` to match the shape of ``other``.\n",
    "        Args:\n",
    "            src (:obj:`torch.Tensor`):\n",
    "                The tensor to broadcast.\n",
    "            other (:obj:`torch.Tensor`):\n",
    "                The tensor to match the shape of.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: The broadcasted tensor.\n",
    "        \"\"\"\n",
    "        for _ in range(src.dim(), other.dim()):\n",
    "            src = src.unsqueeze(-1)\n",
    "        src = src.expand_as(other)\n",
    "        return src\n",
    "\n",
    "    def scatter_sum(src: torch.Tensor, index: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Sums the elements in ``src`` that have the same indices as in ``index``.\n",
    "        Args:\n",
    "            src (:obj:`torch.Tensor`):\n",
    "                The tensor to sum.\n",
    "            index (:obj:`torch.Tensor`):\n",
    "                The indices to sum.\n",
    "        Returns:\n",
    "            :obj:`torch.Tensor`: The summed tensor.\n",
    "        \"\"\"\n",
    "        index = broadcast(index, src)\n",
    "        size = list(src.size())\n",
    "        size[1] = index.max() + 1\n",
    "        print(size)\n",
    "        print(src.dtype)\n",
    "        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n",
    "        return out.scatter_add_(1, index, src)\n",
    "\n",
    "    # replace padding indices with the maximum value inside the batch\n",
    "    indices[indices == -1] = torch.max(indices)\n",
    "    merged = scatter_sum(embeddings, indices)\n",
    "    ones = torch.ones(\n",
    "        indices.size(), dtype=embeddings.dtype, device=embeddings.device\n",
    "    )\n",
    "    count = scatter_sum(ones, indices)\n",
    "    count.clamp_(1)\n",
    "    count = broadcast(count, merged)\n",
    "    merged.true_divide_(count)\n",
    "    return merged[:,:-1,:] #added by me to remove a batch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,None],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install transformers-embedder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "self.vae=VAE.load_from_checkpoint(hparams.vae.pth_folder)\n",
    "self.vae.freeze() #we do not want to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eed754111ede77ce2654f5ed00c707307144a8607362a4447b4b2089c46effd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
