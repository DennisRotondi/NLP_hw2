{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP2022 - Homework 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code for a fast processing of data and experiments execution for the second homework of the course Natural Language Processing 2022. It has been completely wrote by Dennis Rotondi 1834864 using the methodologies learned throughout the course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working with notebook need an 'absolute' import\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports and deterministic stuff\n",
    "import os, sys\n",
    "sys.path.append(os.path.join(\"..\")) #to access hw2 functions\n",
    "sys.path.append(os.path.join(\"../..\")) #to access model folder\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = './nlp_hw2.ipynb' # to avoid a wandb warning\n",
    "os.environ['TOKENIZERS_PARALLELISM'] = \"false\" # to avoid deadlock at traning time for the tokenizer\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from utils import read_dataset\n",
    "import wandb\n",
    "from pytorch_lightning.loggers.wandb import WandbLogger\n",
    "\n",
    "np.random.seed(0)\n",
    "random.seed(0)\n",
    "torch.cuda.manual_seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n",
    "torch.backends.cudnn.benchmark = False\n",
    "_ = pl.seed_everything(0)\n",
    "\n",
    "# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "# these commands allow to update the .py codes imported instead of re-importing everything every time.\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As for the bonus exercise and hw1, I want to start by looking at the data I have to better understand how to proceed in the pre-processing operations. I've read that there are problems with some (sentence-ground truth) pairs, since we are not allowed to do any change I'll directly discharge them for the training phase if needed. I'll do my analysis mostly for the english dataset since it is mandatory and larger \n",
    "\n",
    "(it's possible to reproduce the experiments in different languages just changing the language parameter that follow from \"EN\" to {\"ES\", \"FR\"})."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "language = \"EN\" # \"ES\" or \"FR\" if you want\n",
    "data_file = f\"../../data/{language}/train.json\"\n",
    "\n",
    "sentences, labels = read_dataset(data_file)\n",
    "print(f\"Number of training sentences ({language}): \"+ str(len(sentences.keys())))\n",
    "# I'm just playing with the field of a sentence_id to understand our data samples.\n",
    "sentence_id = list(sentences.keys())[0]\n",
    "print(\"## SENTENCE {} ##\".format(sentence_id))\n",
    "for key in sentences[sentence_id]:\n",
    "    print(key)\n",
    "    print(sentences[sentence_id][key])\n",
    "print(\"## LABEL ##\")\n",
    "for key in labels[sentence_id]:\n",
    "    print(key)\n",
    "    print(labels[sentence_id][key])\n",
    "\n",
    "# let's check and count the different frames and roles\n",
    "verbatlas_frames = Counter()\n",
    "predicate_roles = Counter()\n",
    "pos_tags = Counter()\n",
    "\n",
    "for k in labels:\n",
    "    verbatlas_frames.update(labels[k]['predicates'])\n",
    "    pos_tags.update(sentences[k]['pos_tags'])\n",
    "    for idx in labels[k]['roles']:\n",
    "        predicate_roles.update(labels[k]['roles'][idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"## VF ##\")\n",
    "print(verbatlas_frames)\n",
    "# list of frames in the training dataset\n",
    "l_vf = list(verbatlas_frames.keys())\n",
    "print(l_vf)\n",
    "print(len(l_vf))\n",
    "print(\"## RL ##\")\n",
    "print(predicate_roles)\n",
    "p_r = list(predicate_roles.keys())\n",
    "print(p_r)\n",
    "print(len(p_r))\n",
    "print(\"## PT ##\")\n",
    "ppl = list(pos_tags.keys())\n",
    "print(ppl)\n",
    "print(len(ppl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(predicate_roles[\"_\"])\n",
    "# to place it in a different position and not have overlapping labels\n",
    "predicate_roles[\"experiencer\"] = predicate_roles.pop(\"experiencer\")\n",
    "plt.figure(figsize=(26,10))\n",
    "_ = plt.bar(predicate_roles.keys(), predicate_roles.values()) \n",
    "plt.title(\"Bar Plot of role frequency (without '_' label, 95% of them)\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are clearly not using all the 466 [verbatlas](https://verbatlas.org/) frames but less than 3/4 of them: 303. Working with fewer clusters surely increases the overall performances because the system can only focus on a subset of them. In the next code cell I want to check if in the dev-set I do not have other frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_sentences, dev_labels = read_dataset(f\"../../data/{language}/dev.json\")\n",
    "print(f\"Number of training sentences ({language}): \"+ str(len(dev_sentences.keys())))\n",
    "for k in dev_labels:\n",
    "    verbatlas_frames.update(dev_labels[k]['predicates'])\n",
    "    for idx in dev_labels[k]['roles']:\n",
    "        predicate_roles.update(dev_labels[k]['roles'][idx])\n",
    "\n",
    "l_vf_dev = list(verbatlas_frames.keys())\n",
    "print(len(l_vf_dev))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there are only 4 more frames in the dev_set wrt the train_set, this information is useful for further consideration when I'll deal with the optional part of this homework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I'm starting to understand the samples, it's clear that our dataset does not need much pre-processing, since we already have words tokens and associated lemmas for each sentence. Some more useful statistics are on how long are the sentences on average, how many predicates they have and how the distribution of pos-tagging tokens correlate with roles and predicates. I'll rapidly compute them in what follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mergedeep import merge\n",
    "sentences_length = list()\n",
    "predicates_counter = list()\n",
    "sentences = merge(sentences, labels)\n",
    "pos_pre_corr = Counter()\n",
    "pos_role_corr = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_tags = list(pos_tags.keys())\n",
    "for pt in pos_tags:\n",
    "    pos_pre_corr.update({pt:0})\n",
    "    pos_role_corr.update({pt:0})\n",
    "\n",
    "for s in sentences:\n",
    "    s_l = len(sentences[s][\"lemmas\"])\n",
    "    roles = sentences[s][\"roles\"].keys()\n",
    "    p_c = len(roles)\n",
    "    sentences_length.append(s_l)\n",
    "    predicates_counter.append(p_c)\n",
    "    for pos, predicate in zip(sentences[s][\"pos_tags\"], sentences[s][\"predicates\"]):\n",
    "        if predicate != \"_\":\n",
    "            pos_pre_corr.update({pos:1})\n",
    "    for r in roles:\n",
    "        for pos, role in zip(sentences[s][\"pos_tags\"],sentences[s][\"roles\"][r]):\n",
    "            if role != \"_\":\n",
    "                pos_role_corr.update({pos:1})\n",
    "    \n",
    "sl_np=np.asarray(sentences_length)\n",
    "pc_np=np.asarray(predicates_counter)\n",
    "\n",
    "print(\"Sentences Length\")\n",
    "print(\"mean\", sl_np.mean())\n",
    "print(\"std\", sl_np.std())\n",
    "print(\"min\", sl_np.min())\n",
    "print(\"max\", sl_np.max())\n",
    "\n",
    "print(\"Predicates Counter\")\n",
    "print(\"mean\", pc_np.mean())\n",
    "print(\"std\", pc_np.std())\n",
    "print(\"min\", pc_np.min())\n",
    "print(\"max\", pc_np.max())\n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "_ = plt.hist(sl_np, bins = 'auto') \n",
    "plt.title(\"Histogram of sentences length available\") \n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(8,8)) \n",
    "_ = plt.hist(pc_np, bins = 5) \n",
    "plt.title(\"Histogram of predicate counts for each sentence\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's interesting to notice that (for our EN dataset) there are some sentences with 0 and some with 10 predicates, even if the average is slightly more than 2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26,10))\n",
    "_ = plt.bar(pos_pre_corr.keys(), pos_pre_corr.values()) \n",
    "plt.title(\"Number of predicates for each pos_token\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30\n",
    "\n",
    "print(pos_pre_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A photo is worth a thousand words, if we are able to identify the pos_tag it's also very easy to understand we have a verb or not. (holy grail for task1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(26,10))\n",
    "_ = plt.bar(pos_role_corr.keys(), pos_role_corr.values()) \n",
    "plt.title(\"Number of predicates for each pos_token\") \n",
    "plt.show() #it's possible to notice that most of them are between size 7 and 30\n",
    "\n",
    "print(pos_role_corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also in this case it's clear that there are pos tags that gives more information about which are the arguments, so I'll introduce them in my model if time will allow it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood the importance of having a postag information, taking into account that the input sentence on which my work will be evaluated does not have pos-tag information, I have to retrieve them with an external library. It is important to understand how good is this library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from seqeval.metrics import accuracy_score, f1_score\n",
    "from tqdm import tqdm\n",
    "taggers = {\"EN\":\"en_core_web_sm\", \"ES\":\"es_core_news_sm\", \"FR\":\"fr_core_news_sm\"}\n",
    "nlp = spacy.load(taggers[language])\n",
    "\n",
    "def compute_metrics_postag(field: str):\n",
    "    p_labels = list()\n",
    "    predictions = list()\n",
    "    ppl = list()\n",
    "    pv = list()\n",
    "    for s in tqdm(sentences):\n",
    "        fr = ' '.join(sentences[s][field])\n",
    "        doc = nlp(fr)\n",
    "        for token, pos in zip(doc,sentences[s][\"pos_tags\"]):\n",
    "            predictions.append(token.pos_)\n",
    "            p_labels.append(pos)\n",
    "            if pos ==  \"VERB\":\n",
    "                pv.append(token.pos_)\n",
    "                ppl.append(pos)\n",
    "    acc = accuracy_score([p_labels], [predictions])\n",
    "    f = f1_score([p_labels], [predictions])\n",
    "    print(\"Accuracy, f1 on all the tokens\")\n",
    "    print(acc, f)\n",
    "    # but as seen to solve task 1 we are more interested in identify verb tokens!\n",
    "    accv = accuracy_score([ppl], [pv])\n",
    "    fv = f1_score([ppl], [pv])\n",
    "    print(\"Accuracy on VERB tokens\")\n",
    "    print(accv)\n",
    "\n",
    "compute_metrics_postag(\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we repeat the same experiment computing the pos_tag from the lemmas and NOT the words\n",
    "compute_metrics_postag(\"lemmas\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we can conclude that since acc for verbs with words is much greater than the one with lemmas is better to embed using words. To be precise we are not using the most accurate spacy model for pos-tagging, still this is the most efficient (30mb instead of 400mb) and for this homework I'm not aiming to \"top score\" but to complete different pipelines in a reasonable time due to the fact that working with transformers requires a lot of resources in term of memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to train our model. Pytorch-lightning allows that in such a way that it's easy to modularize everything and train with few lines of code all the different models. Moreover using wandb as logger I auto-plot the training evolution in high quality plots and it's also possible to save the training history of the different trials. This will be very useful for comparing the experiments in the report.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 256,\n",
      " 'bidirectional': True,\n",
      " 'dropout': 0.3,\n",
      " 'embedding_dim': 768,\n",
      " 'hidden_dim': 400,\n",
      " 'language': 'EN',\n",
      " 'language_model_name': 'bert-base-uncased',\n",
      " 'lr': 0.001,\n",
      " 'n_cpu': 8,\n",
      " 'need_train': True,\n",
      " 'num_layers': 1,\n",
      " 'pos_tag_emb_dim': 232,\n",
      " 'pos_tag_tokens': 17,\n",
      " 'role_classes': 27,\n",
      " 'task': '1234',\n",
      " 'trainable_embeddings': True,\n",
      " 'wd': 0}\n"
     ]
    }
   ],
   "source": [
    "from datasets_srl import SRL_DataModule\n",
    "from implementation import HParams, SRL_34, SRL_1234\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "from utils import read_dataset, evaluate_argument_classification, evaluate_argument_identification\n",
    "from mergedeep import merge\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "# these are some parameters that allow as I said to modularize the training. We need to store the hypermarameters of the model (lr, wd, ...), the language\n",
    "# and the task on which we want to perform the training.\n",
    "hparams = HParams()\n",
    "languages = [\"EN\", \"ES\", \"FR\"]\n",
    "tasks = [\"34\", \"234\", \"1234\"]\n",
    "models = {\"34\": SRL_34, \"1234\": SRL_1234}\n",
    "\n",
    "language = languages[0]\n",
    "task = tasks[2]\n",
    "epochs = 100\n",
    "SRL_Model = models[task]\n",
    "hparams.language = language\n",
    "hparams.task = task\n",
    "hparams = asdict(hparams)\n",
    "pprint(hparams)\n",
    "# after reading the dataset I merge the two dicts (sentences and labels) since there is a field in common (predicate)\n",
    "# and it's only a waste of space keeping it in memory 2 copies of it.\n",
    "sentences = merge(*read_dataset(\"../../data/\"+language+\"/train.json\"))\n",
    "sentences_test = merge(*read_dataset(\"../../data/\"+language+\"/dev.json\"))\n",
    "\n",
    "data = SRL_DataModule(hparams, task, language, sentences, sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/dennis/Applications/anaconda3/envs/nlp2022-hw2/lib/python3.9/site-packages/pytorch_lightning/loggers/wandb.py:347: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "model = SRL_Model(hparams=hparams, sentences_for_evaluation=sentences_test)\n",
    "# Define the logger\n",
    "# https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "# NOTE: to use wandb properly you need to login in wandb (need an account) \n",
    "# or use a different logger eg. TensorBoard, I'm used to this one so I'll go for it.\n",
    "# to login: https://docs.wandb.ai/ref/cli/wandb-login\n",
    "wandb.require(\"service\")\n",
    "wandb_logger = WandbLogger(project=\"SRL_\"+task, log_model = True) # note not language in the project name, so we can compare different languages\n",
    "wandb_logger.experiment.watch(model, log = 'False', log_freq = 100000)\n",
    "# Define the trainer\n",
    "metric_to_monitor =  \"f1\" # f1 of argument classification, also possible eg 'avg_val_loss'\n",
    "mode = \"max\" #you want to maximixe or minimize the metric?\n",
    "# we employ the early stopping technique to avoid hours of usuless training, pl gives it for free\n",
    "early_stop_callback = EarlyStopping(monitor = metric_to_monitor, min_delta = 0.00, patience = 10, verbose = True, mode = mode)\n",
    "# it is also useful to keep track of the best model during the epochs (if you remember I did all this manually last hw)or use a different logger,\n",
    "# we have a callback even for this.\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "                        save_top_k = 1,\n",
    "                        monitor = metric_to_monitor,\n",
    "                        mode = mode,\n",
    "                        dirpath = \"../../model\",\n",
    "                        filename = \"SRL_\"+task+\"_\"+language+\"-{epoch:02d}-{f1:.4f}\",\n",
    "                        verbose = True\n",
    "                    )\n",
    "# the trainer collect all the useful informations so far for the training \n",
    "trainer = pl.Trainer(logger = wandb_logger,\n",
    "                    max_epochs = epochs, \n",
    "                    gpus = 1,\n",
    "                    callbacks = [early_stop_callback, checkpoint_callback])    \n",
    "\n",
    "save_ckpt_file = \"../../model/SRL_{}_{}_last.ckpt\".format(task, language)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start the training without initialized weights, if you want to inizialize them skip this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.fit(model, data)\n",
    "trainer.save_checkpoint(save_ckpt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To continue the training it's possible to just increase the number of epochs and create a new trainer, also possible to fine tune the model for another language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_ckpt = save_ckpt_file # or use another language / pre-trained model.\n",
    "epochs += 10 # increase the maximum number of epochs\n",
    "trainer = pl.Trainer(logger = wandb_logger,\n",
    "                    max_epochs = epochs, \n",
    "                    gpus = 1,\n",
    "                    callbacks = [early_stop_callback, checkpoint_callback],\n",
    "                    resume_from_checkpoint = resume_ckpt)    \n",
    "trainer.fit(model, data)\n",
    "trainer.save_checkpoint(save_ckpt_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have trained for enough epochs you can now finish logging with wandb to have your plot.\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to upload online the run after finishing (you will have a string like the one below, just execute in on terminal or here)\n",
    "!wandb sync /home/dennis/Desktop/nlp2022-hw2/hw2/stud/wandb/offline-run-20220712_105118-2uum41pe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix Analysis for argument classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our trained model it's interesting to understand where it performs better and where not. Recalling that from the english dataset analysis these were the numbers of different roles:\n",
    "\n",
    "{'_': 437392, 'agent': 7581, 'theme': 6593, 'patient': 2907, 'goal': 1463, 'topic': 1403, 'recipient': 837, 'beneficiary': 590, 'result': 577, 'stimulus': 367, 'experiencer': 319, 'attribute': 294, 'destination': 276, 'co-theme': 253, 'source': 229, 'location': 198, 'co-agent': 145, 'product': 95, 'instrument': 70, 'co-patient': 60, 'extent': 54, 'cause': 51, 'value': 45, 'time': 35, 'asset': 28, 'purpose': 25, 'material': 11}\n",
    "\n",
    "by inspection could be possible to estimate a correlation between the number of samples and the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import and utils for this evaluation\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from typing import Dict, List, Any\n",
    "from utils import evaluate_argument_classification, evaluate_argument_identification\n",
    "\n",
    "def flat_dict_roles(sentences: Dict[str, Dict[str, List[str]]]) -> List[Any]:\n",
    "    list_tokens = list()\n",
    "    for s in sentences:\n",
    "        for l in sentences[s][\"roles\"]:\n",
    "            list_tokens+=[token for token in sentences[s][\"roles\"][l]]\n",
    "    return list_tokens\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_ckpt = \"../../model/SRL_34_EN-epoch=51-f1=0.8561.ckpt\"\n",
    "# we load weights in a not strict fashion since I made some adjustment to the models during the way \n",
    "model = SRL_34.load_from_checkpoint(best_ckpt, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = model.to(device).predict(sentences_test, require_ids=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AI\")\n",
    "print(evaluate_argument_identification(sentences_test, predict))\n",
    "print(\"AC\")\n",
    "print(evaluate_argument_classification(sentences_test, predict))\n",
    "\n",
    "flat_labels_s = flat_dict_roles(sentences_test)\n",
    "flat_predictions_s = flat_dict_roles(predict)\n",
    "\n",
    "all_labels = ['_', 'agent', 'theme', 'beneficiary', 'patient', 'topic', 'goal', 'recipient', 'co-theme', 'result', \\\n",
    "    'stimulus', 'experiencer', 'destination', 'value', 'attribute', 'location', 'source', 'cause', 'co-agent', \\\n",
    "    'time', 'co-patient', 'product', 'purpose', 'instrument', 'extent', 'asset', 'material']\n",
    "\n",
    "cm = confusion_matrix(flat_labels_s, flat_predictions_s, labels=all_labels, normalize=\"true\")\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=all_labels)\n",
    "fig, ax = plt.subplots(figsize=(36,36))\n",
    "disp.plot(ax=ax) #7162"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected the fewer the samples the worst the performances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOREMOVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets_srl import SRL_DataModule\n",
    "from implementation import HParams, SRL_34\n",
    "from dataclasses import dataclass, asdict\n",
    "from pprint import pprint\n",
    "from utils import read_dataset, evaluate_argument_classification, evaluate_argument_identification\n",
    "from mergedeep import merge\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "while True:\n",
    "    # these are some parameters that allow as I said to modularize the training. We need to store the hypermarameters of the model (lr, wd, ...), the language\n",
    "    # and the task on which we want to perform the training.\n",
    "    hparams = asdict(HParams())\n",
    "    pprint(hparams)\n",
    "    languages = [\"EN\", \"ES\", \"FR\"]\n",
    "    tasks = [\"34\", \"234\", \"1234\"]\n",
    "    models = {\"34\": SRL_34}\n",
    "    language = languages[0]\n",
    "    task = tasks[0]\n",
    "    epochs = 100\n",
    "    SRL_Model = models[task]\n",
    "    # after reading the dataset I merge the two dicts (sentences and labels) since there is a field in common (predicate)\n",
    "    # and it's only a waste of space keeping it in memory 2 copies of it.\n",
    "    sentences = merge(*read_dataset(\"../../data/\"+language+\"/train.json\"))\n",
    "    sentences_test = merge(*read_dataset(\"../../data/\"+language+\"/dev.json\"))\n",
    "    data = SRL_DataModule(hparams, task, language, sentences, sentences_test)\n",
    "    model = SRL_Model(hparams=hparams, sentences_for_evaluation=sentences_test)\n",
    "    # Define the logger\n",
    "    # https://www.wandb.com/articles/pytorch-lightning-with-weights-biases.\n",
    "    # NOTE: to use wandb properly you need to login in wandb (need an account) \n",
    "    # or use a different logger eg. TensorBoard, I'm used to this one so I'll go for it.\n",
    "    # to login: https://docs.wandb.ai/ref/cli/wandb-login\n",
    "    wandb.require(\"service\")\n",
    "    wandb_logger = WandbLogger(project=\"SRL_\"+task, log_model = True) # note not language in the project name, so we can compare different languages\n",
    "    wandb_logger.experiment.watch(model, log = 'False', log_freq = 100000)\n",
    "    # Define the trainer\n",
    "    metric_to_monitor =  \"f1\" # f1 of argument classification, also possible eg 'avg_val_loss'\n",
    "    mode = \"max\" #you want to maximixe or minimize the metric?\n",
    "    # we employ the early stopping technique to avoid hours of usuless training, pl gives it for free\n",
    "    early_stop_callback = EarlyStopping(monitor = metric_to_monitor, min_delta = 0.00, patience = 10, verbose = True, mode = mode)\n",
    "    # it is also useful to keep track of the best model during the epochs (if you remember I did all this manually last hw)or use a different logger,\n",
    "    # we have a callback even for this.\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "                            save_top_k = 1,\n",
    "                            monitor = metric_to_monitor,\n",
    "                            mode = mode,\n",
    "                            dirpath = \"../../model\",\n",
    "                            filename = \"SRL_\"+task+\"_\"+language+\"-{epoch:02d}-{f1:.4f}\",\n",
    "                            verbose = True\n",
    "                        )\n",
    "    # the trainer collect all the useful informations so far for the training \n",
    "    trainer = pl.Trainer(logger = wandb_logger,\n",
    "                        max_epochs = epochs, \n",
    "                        gpus = 1,\n",
    "                        callbacks = [early_stop_callback, checkpoint_callback])    \n",
    "\n",
    "    trainer.fit(model, data)\n",
    "    wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.load(\"../../model/srl_34_EN.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence = \"Using a Transformer network is simple\"\n",
    "sequence2 = \"we ciao\"\n",
    "# tokens = tokenizer.tokenize(sequence)\n",
    "\n",
    "# # print(tokens)\n",
    "\n",
    "# ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# # print(ids)\n",
    "\n",
    "tokenized_inputs = tokenizer([[sequence, \"simple\"],[sequence2, \"ciao\"]],padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens\n",
    "\n",
    "print(tokenized_inputs)\n",
    "print(tokenized_inputs['input_ids'].shape)\n",
    "print(tokenized_inputs.word_ids(0))\n",
    "print(tokenized_inputs.word_ids(1))\n",
    "# NOTE: in the dataset use those word ids to average and simply filter for example... MAY NOT WORK:...\n",
    "sequence3 = [\"we\", \"ciao\"]\n",
    "print(\"second - use this one!!\")\n",
    "tokenized_inputs2 = tokenizer.batch_encode_plus([([sequence.split(), [\"simple\"]]),(sequence3,[\"ciao\"])], add_special_tokens=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens\n",
    "print(tokenized_inputs2)\n",
    "print(tokenized_inputs2['input_ids'].shape)\n",
    "print(tokenized_inputs2.word_ids(0))\n",
    "a, b, c = tokenizer.batch_encode_plus([([sequence.split(), [\"simple\"]]),(sequence3,[\"ciao\"])], add_special_tokens=True, is_split_into_words=True, padding=True, return_tensors=\"pt\")  # \"pt\" -> return PyTorch torch.Tensor objects, rather than a list of tokens \n",
    "print(\"aaa\")\n",
    "print(b)\n",
    "print(\"aaa\")\n",
    "# print(tokenized_inputs2.word_ids(1))\n",
    "# sequence_a = \"This is a short sequence.\"\n",
    "# sequence_b = \"This is a rather long sequence. It is at least longer than the sequence A.\"\n",
    "# print(\"test\")\n",
    "# padded_sequences = tokenizer([sequence_a, sequence_b], padding=True)\n",
    "# print(padded_sequences)\n",
    "\n",
    "transformers_outputs = auto_model(**tokenized_inputs)#['input_ids']\n",
    "# print(transformers_outputs)\n",
    "transformers_outputs_sum = torch.stack(transformers_outputs.hidden_states[-4:], dim=0).sum(dim=0)\n",
    "print(transformers_outputs_sum.shape)\n",
    "# I should remove 2 sep and 1 cls, 1 additional token -> final size 7\n",
    "\n",
    "\n",
    "\n",
    "# filter_toke = tokenized_inputs['input_ids'][:, 1:-3, ...]\n",
    "# print(filter_toke.shape)\n",
    "# labels  = tokenized_inputs.word_ids()[1:-3]\n",
    "# samp_size = filter_toke.shape[1]\n",
    "# M = torch.zeros(max(labels)+1, samp_size)\n",
    "# M[labels, torch.arange(samp_size)] = 1\n",
    "# print(M)\n",
    "# M = torch.nn.functional.normalize(M, p=1, dim=1)\n",
    "# print(M)\n",
    "# torch.mm(M, filter_toke[0]).shape\n",
    "\n",
    "# i want to have an ID to unde\n",
    "# item[\"role_id\"] = (item[\"role_labels\"] == self.labels_to_id[\"_\"]).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = torch.Tensor([[\n",
    "                     [0.1, 0.1],    #-> group / class 1\n",
    "                     [0.2, 0.2],    #-> group / class 2\n",
    "                     [0.4, 0.4],    #-> group / class 2\n",
    "                     [0.0, 0.0]     #-> group / class 0\n",
    "              ],\n",
    "              [\n",
    "                     [0.1, 0.1],    #-> group / class 1\n",
    "                     [0.2, 0.2],    #-> group / class 1\n",
    "                     [0.0, 0.0],    #-> group / class 0\n",
    "                     [12.0, 12.0]   #-> group / class 0\n",
    "              ]])\n",
    "\n",
    "from transformers_embedder.embedder import TransformersEmbedder\n",
    "labels = torch.LongTensor([[1, 2, 2, 2],[1,2,0,2]])\n",
    "\n",
    "\n",
    "print(TransformersEmbedder.merge_scatter(samples, labels))\n",
    "print(TransformersEmbedder.merge_scatter(samples, labels).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.tensor([1,2,None],dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()\n",
    "\n",
    "self.vae=VAE.load_from_checkpoint(hparams.vae.pth_folder)\n",
    "self.vae.freeze() #we do not want to train it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers_embedder as tre\n",
    "from transformers import AutoModel\n",
    "from transformers import AutoTokenizer\n",
    "# tokenizer = tre.Tokenizer(\"bert-base-cased\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "auto_model = AutoModel.from_pretrained(\"bert-base-uncased\", output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in auto_model.encoder.layer[11].parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = [\n",
    "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"1\"],\n",
    "    [\"This\", \"is\", \"sample\", \"sentence\", \"2\"],\n",
    "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"3\"],\n",
    "    # ...\n",
    "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"n\", \"for\", \"batch\"],\n",
    "]\n",
    "\n",
    "batch_pair = [\n",
    "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"pair\", \"1\"],\n",
    "    [\"This\", \"is\", \"sample\", \"sentence\", \"pair\", \"2\"],\n",
    "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"pair\", \"3\"],\n",
    "    # ...\n",
    "    [\"This\", \"is\", \"a\", \"sample\", \"sentence\", \"pair\", \"n\", \"for\", \"batch\"],\n",
    "]\n",
    "tokenizer(batch, batch_pair, padding=True, return_tensors=\"pt\", is_split_into_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "emb = nn.Embedding(18, 32, padding_idx=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb(torch.tensor(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "sents = ['Hello', ',','world', '.']\n",
    "doc = Doc(nlp.vocab, sent)\n",
    "for token in nlp(doc):\n",
    "    print(token.text, token.pos_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.Sigmoid()\n",
    "loss = nn.BCELoss()\n",
    "input = torch.randn(3, requires_grad=True)\n",
    "target = torch.empty(3).random_(2)\n",
    "output = loss(m(input), target)\n",
    "print(output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('nlp2022-hw2')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4eed754111ede77ce2654f5ed00c707307144a8607362a4447b4b2089c46effd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
